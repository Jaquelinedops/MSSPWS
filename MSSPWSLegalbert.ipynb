{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a99c0-56de-4131-81bc-3f85eea699c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import os\n",
    "import urllib.request\n",
    "import transformers\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)  # Add logging\n",
    "print(torch.cuda.is_available())           \n",
    "print(torch.cuda.get_device_name(0))        \n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Total VRAM:\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"CUDA not available. GPU not detected.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# ----------------------\n",
    "# 0. Load and Clean Dataset\n",
    "# ----------------------\n",
    "df = pd.read_csv('C:/Users/Jaque/Downloads/modern_slavery_NER_us_india_val1.csv', index_col=0)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def get_only_words_from_strings(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return re.sub(r'\\d+', '', text).strip()\n",
    "\n",
    "df['content_corrected'] = df['content_corrected'].apply(get_only_words_from_strings)\n",
    "df = df[['content_corrected', 'modern_slavery_in_supply_chain']].dropna()\n",
    "df['target'] = df['modern_slavery_in_supply_chain'].apply(lambda x: 1 if str(x).strip().lower() == 'yes' else 0)\n",
    "df = df[['content_corrected', 'target']]\n",
    "\n",
    "texts = df['content'].tolist()\n",
    "labels = df['target'].tolist()\n",
    "\n",
    "# ----------------------\n",
    "# 1. Split Data\n",
    "# ----------------------\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ----------------------\n",
    "# 2. Tokenization\n",
    "# ----------------------\n",
    "MODEL_NAME = \"casehold/legalbert\"  # SPECIFICALLY casehold/legalbert\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# --- CACHE MANAGEMENT ---\n",
    "cache_dir = \"./legalbert_cache\"  # Explicit cache directory (create it if it doesn't exist)\n",
    "os.makedirs(cache_dir, exist_ok=True)  # Ensure the directory exists\n",
    "print(f\"Transformers cache directory: {cache_dir}\")\n",
    "\n",
    "# --- NETWORK CHECK (Simplified) ---\n",
    "try:\n",
    "    urllib.request.urlopen('https://www.google.com', timeout=5)\n",
    "    print(\"Network connection OK\")\n",
    "except urllib.error.URLError as e:\n",
    "    print(f\"Network error: {e}\")\n",
    "\n",
    "# --- VERSION CHECK ---\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# --- TOKENIZER and MODEL LOADING ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir, force_download=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, cache_dir=cache_dir, force_download=True)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors='tf')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors='tf')\n",
    "\n",
    "# ----------------------\n",
    "# 3. Prepare TensorFlow Datasets\n",
    "# ----------------------\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).shuffle(len(train_texts)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(BATCH_SIZE)\n",
    "\n",
    "# ----------------------\n",
    "# 4. Compile Model\n",
    "# ----------------------\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# ----------------------\n",
    "# 5. Train the Model with Timer\n",
    "# ----------------------\n",
    "start_train = time.time()\n",
    "model.fit(train_dataset, validation_data=test_dataset, epochs=EPOCHS)\n",
    "end_train = time.time()\n",
    "print(f\"\\nðŸ•’ Training time: {end_train - start_train:.2f} seconds\")\n",
    "\n",
    "# ----------------------\n",
    "# 6. Evaluate and Analyze Results\n",
    "# ----------------------\n",
    "start_eval = time.time()\n",
    "results = model.evaluate(test_dataset, verbose=0)\n",
    "end_eval = time.time()\n",
    "print(f\"ðŸ•’ Evaluation time: {end_eval - start_eval:.2f} seconds\")\n",
    "\n",
    "acc = results[1]\n",
    "predictions = model.predict(test_dataset)\n",
    "y_pred_logits = predictions.logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = np.array(test_labels)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# ----------------------\n",
    "# 7. Store Results\n",
    "# ----------------------\n",
    "model_display_name = f'casehold/legalbert'  # Accurate model name\n",
    "final_results = {\n",
    "    'Model': model_display_name,\n",
    "    'Accuracy': round(acc * 100, 4),\n",
    "    'Precision': round(prec * 100, 4),\n",
    "    'Recall': round(rec * 100, 4),\n",
    "    'F1 Score': round(f1 * 100, 4),\n",
    "}\n",
    "\n",
    "if 'test_result' not in locals():\n",
    "    test_result = pd.DataFrame()\n",
    "test_result = pd.concat([test_result, pd.DataFrame([final_results])], ignore_index=True)\n",
    "\n",
    "print(\"\\nðŸ“Š Updated Test Results DataFrame:\")\n",
    "print(test_result)\n",
    "\n",
    "# ----------------------\n",
    "# 8. Save Fine-tuned Model and Tokenizer\n",
    "# ----------------------\n",
    "save_directory = \"./casehold_legalbert_finetuned\"  # Specific save directory\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"\\nðŸ’¾ Model and tokenizer saved to: {save_directory}\")\n",
    "\n",
    "# ----------------------\n",
    "# 9. Optional Inference Function\n",
    "# ----------------------\n",
    "def predict(text):\n",
    "    start_pred = time.time()\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=MAX_LEN)\n",
    "    logits = model(inputs).logits\n",
    "    end_pred = time.time()\n",
    "\n",
    "    prob = tf.nn.softmax(logits, axis=1).numpy()[0]\n",
    "    label = tf.argmax(logits, axis=1).numpy()[0]\n",
    "\n",
    "    print(f\"ðŸ•’ Prediction time: {end_pred - start_pred:.4f} seconds\")\n",
    "    return {\"predicted_class\": int(label), \"probability_yes\": float(prob[1]), \"probability_no\": float(prob[0])}\n",
    "\n",
    "# Example\n",
    "print(\"\\nðŸ§ª Example prediction:\")\n",
    "print(predict(\"The company failed to monitor labor practices in its supply chain.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86f981-8f10-4890-86c1-e32037e0cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-env3]",
   "language": "python",
   "name": "conda-env-anaconda3-env3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
