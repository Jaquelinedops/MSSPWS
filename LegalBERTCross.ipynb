{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import os\n",
    "import torch # Apenas para verifica√ß√£o de GPU, se dispon√≠vel\n",
    "\n",
    "# --- 0. VERIFICA√á√ÉO DE GPU (Do seu snippet original) ---\n",
    "print(\"--- Verifica√ß√£o de Hardware ---\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Total VRAM:\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"CUDA not available. GPU not detected via Torch (TF usar√° o que estiver dispon√≠vel).\")\n",
    "\n",
    "# --- CORRE√á√ÉO DE COMPATIBILIDADE (TODOS OS COMPONENTES) ---\n",
    "try:\n",
    "    from tf_keras.optimizers import Adam\n",
    "    from tf_keras.losses import SparseCategoricalCrossentropy\n",
    "    from tf_keras.callbacks import EarlyStopping\n",
    "    print(\"‚úÖ Usando tf_keras para compatibilidade total.\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        from tensorflow.keras.optimizers.legacy import Adam\n",
    "        from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        print(\"‚ö†Ô∏è tf_keras n√£o encontrado. Usando tensorflow.keras.optimizers.legacy.\")\n",
    "    except ImportError:\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        print(\"‚ö†Ô∏è Usando tensorflow.keras padr√£o.\")\n",
    "\n",
    "# --- CONFIGURA√á√ïES DO LEGALBERT ---\n",
    "MODEL_NAME = \"casehold/legalbert\"\n",
    "MAX_LENGTH = 256 # LegalBERT aguenta contexto maior, mantive 256 do seu snippet\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000 \n",
    "PATIENCE = 3  \n",
    "LEARNING_RATE = 2e-5\n",
    "CACHE_DIR = \"./legalbert_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# --- FUN√á√ÉO CI ---\n",
    "def ci95(values):\n",
    "    values = np.array(values)\n",
    "    mean = np.mean(values)\n",
    "    if len(values) <= 1: return np.nan, np.nan \n",
    "    se = stats.sem(values) \n",
    "    h = 1.96 * se\n",
    "    return mean - h, mean + h\n",
    "\n",
    "# --- 1. CARREGAMENTO DE DADOS (Igual ao RoBERTa CV) ---\n",
    "if not os.path.exists('kfolds_resampled_indices.pkl'):\n",
    "    print(\"‚ùå ERRO: Arquivo 'kfolds_resampled_indices.pkl' n√£o encontrado.\")\n",
    "else:\n",
    "    with open('kfolds_resampled_indices.pkl', 'rb') as f:\n",
    "        loaded_kfolds_indices = pickle.load(f)\n",
    "    print(\"‚úÖ √çndices K-Fold carregados.\")\n",
    "\n",
    "# --- 2. PREPARA√á√ÉO DOS DADOS ---\n",
    "# O script assume que os dados j√° est√£o carregados no ambiente ou no mesmo diret√≥rio\n",
    "# Se estiver rodando do zero, precisa carregar o CSV resampled aqui\n",
    "if 'X_train_resampled' not in locals() or 'y_train_resampled' not in locals():\n",
    "    if os.path.exists('train_resampled_full.csv'):\n",
    "        print(\"Carregando 'train_resampled_full.csv'...\")\n",
    "        df_train_resampled = pd.read_csv('train_resampled_full.csv')\n",
    "        # Ajuste o nome da coluna de texto se necess√°rio (ex: 'content_corrected')\n",
    "        col_text = 'content_corrected' if 'content_corrected' in df_train_resampled.columns else 'content'\n",
    "        X_train_resampled = df_train_resampled[col_text]\n",
    "        y_train_resampled = df_train_resampled['target']\n",
    "    else:\n",
    "        raise ValueError(\"ERRO: Vari√°veis X_train_resampled/y_train_resampled n√£o encontradas e CSV n√£o achado.\")\n",
    "\n",
    "texts = X_train_resampled.astype(str).values\n",
    "labels = y_train_resampled.values\n",
    "\n",
    "# --- 3. TOKENIZA√á√ÉO GLOBAL ---\n",
    "print(f\"--- Tokenizando dados para CV com {MODEL_NAME} ---\")\n",
    "# Usando AutoTokenizer para LegalBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "encodings = tokenizer(\n",
    "    texts.tolist(),\n",
    "    max_length=MAX_LENGTH,\n",
    "    padding='max_length', # Ou 'max_length' para garantir formato fixo no numpy\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "input_ids_all = encodings['input_ids'].numpy()\n",
    "attention_mask_all = encodings['attention_mask'].numpy()\n",
    "\n",
    "# --- 4. LOOP DE VALIDA√á√ÉO CRUZADA ---\n",
    "cv_metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': []\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Iniciando Valida√ß√£o Cruzada ({len(loaded_kfolds_indices)} Folds) ---\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(loaded_kfolds_indices):\n",
    "    print(f\"\\nüîÑ Training Fold {fold + 1}/{len(loaded_kfolds_indices)}...\")\n",
    "    \n",
    "    # A. Limpar sess√£o\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # B. Separar dados\n",
    "    X_train_ids = input_ids_all[train_idx]\n",
    "    X_train_mask = attention_mask_all[train_idx]\n",
    "    y_train_fold = labels[train_idx]\n",
    "    \n",
    "    X_val_ids = input_ids_all[val_idx]\n",
    "    X_val_mask = attention_mask_all[val_idx]\n",
    "    y_val_fold = labels[val_idx]\n",
    "    \n",
    "    # C. Datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': X_train_ids, 'attention_mask': X_train_mask}, y_train_fold)\n",
    "    ).shuffle(1000).batch(BATCH_SIZE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': X_val_ids, 'attention_mask': X_val_mask}, y_val_fold)\n",
    "    ).batch(BATCH_SIZE)\n",
    "    \n",
    "    # D. Instanciar Modelo (AutoModel para LegalBERT)\n",
    "    # Detectar n√∫mero de classes dinamicamente\n",
    "    num_classes = len(np.unique(labels))\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=num_classes,\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "    \n",
    "    # Configurar Componentes\n",
    "    optimizer_inst = Adam(learning_rate=LEARNING_RATE)\n",
    "    loss_inst = SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    model.compile(optimizer=optimizer_inst, loss=loss_inst, metrics=['accuracy'])\n",
    "    \n",
    "    # E. Treinar\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        mode='max', \n",
    "        patience=PATIENCE, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # F. Avaliar\n",
    "    predictions = model.predict(val_dataset)\n",
    "    y_pred_logits = predictions.logits\n",
    "    y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_val_fold, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    cv_metrics['accuracy'].append(acc)\n",
    "    cv_metrics['precision'].append(prec)\n",
    "    cv_metrics['recall'].append(rec)\n",
    "    cv_metrics['f1'].append(f1)\n",
    "    \n",
    "    print(f\"‚úÖ Fold {fold+1} Result -> Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# --- 5. RESULTADOS FINAIS ---\n",
    "print(f\"\\n--- üìä Resultados Consolidados ({MODEL_NAME}) ---\")\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for metric_name, values in cv_metrics.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    ci_lower, ci_upper = ci95(values)\n",
    "    \n",
    "    results_summary[metric_name] = {\n",
    "        'Mean': mean_val,\n",
    "        'Std': std_val,\n",
    "        'CI_Lower': ci_lower,\n",
    "        'CI_Upper': ci_upper\n",
    "    }\n",
    "    \n",
    "    print(f\"{metric_name.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f} (95% CI: [{ci_lower:.4f}, {ci_upper:.4f}])\")\n",
    "\n",
    "df_results_cv = pd.DataFrame(results_summary).T\n",
    "print(\"\\nTabela de Resultados CV:\")\n",
    "print(df_results_cv)\n",
    "\n",
    "# Salvar\n",
    "df_results_cv.to_csv('resultados_validacao_cruzada_legalbert.csv')\n",
    "print(\"\\nResultados salvos em 'resultados_validacao_cruzada_legalbert.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (destil-ner)",
   "language": "python",
   "name": "destil-ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
