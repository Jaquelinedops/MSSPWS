{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaque\\AppData\\Local\\Temp\\ipykernel_45732\\1819983139.py:38: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import GridSearch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Jaque\\anaconda3\\envs\\new_env_modern_slavery\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jaque\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Ti\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Ti\n",
      "Total VRAM: 11.99 GB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Dense, SpatialDropout1D,\n",
    "    Bidirectional, Attention, GlobalAveragePooling1D, Dropout,\n",
    "    Layer,  # Import the Layer class\n",
    "    Multiply # Import the Multiply layer\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "import numpy as np\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Dropout, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperParameters, GridSearch, Objective\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from kerastuner import GridSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner import Objective # Import directly from kerastuner\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from kerastuner.tuners import GridSearch  # Import GridSearch tuner\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "# Download required NLTK data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import time\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming your data is in a pandas DataFrame\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder # If labels are strings\n",
    "from imblearn.under_sampling import RandomUnderSampler #added\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, make_scorer, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,  StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "path_source_code = ''\n",
    "from urllib.parse import urlparse\n",
    "import tqdm\n",
    "from xgboost import XGBClassifier  # Make sure this import is included\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Dropout, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperParameters, GridSearch, Objective\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import sys\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sys.path.append(path_source_code) # Replace with the actual path to the directory containing pdf_preprosseing.py\n",
    "from pre_processing import get_only_words_from_strings, remove_stop_words, extract_entities,  hash_content, correct_text,plot_confusion_matrix, extract_unique_values, replace_entity, detect_words_segment , find_emails,find_links,extract_radical, remove_numbers , identify_frequent_short_words\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "from scipy.sparse import issparse\n",
    "import joblib\n",
    "import random\n",
    "# Set random seeds\n",
    "\n",
    "import torch\n",
    "# ---- Load NER models ----\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "\n",
    "print(torch.cuda.is_available())            # ‚úÖ Should be True\n",
    "print(torch.cuda.get_device_name(0))        # Should show your GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Total VRAM:\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"CUDA not available. GPU not detected.\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_entities(doc):\n",
    "    text = doc.text\n",
    "    offset = 0\n",
    "    orgs, persons, locations, dates, gpes = [], [], [], [], []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        start = ent.start_char + offset\n",
    "        end = ent.end_char + offset\n",
    "        replacement = f\"[[{label}]]\"\n",
    "\n",
    "        # Replace in text\n",
    "        text = text[:start] + replacement + text[end:]\n",
    "        offset += len(replacement) - (end - start)\n",
    "\n",
    "        # Collect entities by type\n",
    "        if label == \"ORG\":\n",
    "            orgs.append(ent.text)\n",
    "        elif label == \"PERSON\":\n",
    "            persons.append(ent.text)\n",
    "        elif label == \"LOC\":\n",
    "            locations.append(ent.text)\n",
    "        elif label == \"DATE\":\n",
    "            dates.append(ent.text)\n",
    "        elif label == \"GPE\":\n",
    "            gpes.append(ent.text)\n",
    "\n",
    "    return text, orgs, persons, locations, dates, gpes\n",
    "\n",
    "\n",
    "\n",
    "def identity_entities(batch):\n",
    "    ner_results = date_ner(batch[\"content_no_entity\"])  # Use RoBERTa pipeline\n",
    "\n",
    "    new_texts, orgs_list, pers_list, locs_list, miscs_list, dates_list = [], [], [], [], [], []\n",
    "\n",
    "    for i, text in enumerate(batch[\"content_no_entity\"]):\n",
    "        ents = ner_results[i]\n",
    "\n",
    "        dates = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"DATE\"]\n",
    "        orgs  = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"ORG\"]\n",
    "        pers  = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"PER\"]\n",
    "        locs  = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"LOC\"]\n",
    "        miscs = [e[\"word\"] for e in ents if e[\"entity_group\"] == \"MISC\"]\n",
    "\n",
    "        # Replace entities\n",
    "        anonymized = text\n",
    "        anonymized = replace_entity(anonymized, dates, \" [[DATE]] \")\n",
    "        anonymized = replace_entity(anonymized, orgs,  \" [[ORG]] \")\n",
    "        anonymized = replace_entity(anonymized, pers,  \" [[PER]] \")\n",
    "        anonymized = replace_entity(anonymized, locs,  \" [[LOC]] \")\n",
    "        anonymized = replace_entity(anonymized, miscs, \" [[MISC]] \")\n",
    "\n",
    "        # Append results\n",
    "        new_texts.append(anonymized)\n",
    "        orgs_list.append(list(set(orgs)))\n",
    "        pers_list.append(list(set(pers)))\n",
    "        locs_list.append(list(set(locs)))\n",
    "        miscs_list.append(list(set(miscs)))\n",
    "        dates_list.append(list(set(dates)))\n",
    "\n",
    "    return {\n",
    "        \"content_no_entity\": new_texts,\n",
    "        \"org\": orgs_list,\n",
    "        \"person\": pers_list,\n",
    "        \"location\": locs_list,\n",
    "        \"misc\": miscs_list,\n",
    "        \"date\": dates_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment\n",
    "Processador\t13th Gen Intel(R) Core(TM) i9-13900K   3.00 GHz\n",
    "24 Cores\n",
    "RAM instalada\t64,0 GB (utiliz√°vel: 63,7 GB)\n",
    "Tipo de sistema\tSistema operacional de 64 bits, processador baseado em x64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m##Data cleaning and preparation \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m file_path = \u001b[43mos\u001b[49m.path.join(path_source_code, \u001b[33m\"\u001b[39m\u001b[33mmodern_slavery_ENTITIES_related_us_india.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m file_path2 = os.path.join(path_source_code, \u001b[33m\"\u001b[39m\u001b[33mmodern_slavery_NER_us_india.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os.path.exists(file_path) \u001b[38;5;129;01mand\u001b[39;00m os.path.exists(file_path2)):\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "##Data cleaning and preparation \n",
    "file_path = os.path.join(path_source_code, \"modern_slavery_ENTITIES_related_us_india.csv\")\n",
    "file_path2 = os.path.join(path_source_code, \"modern_slavery_NER_us_india.csv\")\n",
    "if not (os.path.exists(file_path) and os.path.exists(file_path2)):\n",
    "    df = pd.read_csv('./modern_slavery_NER_us_india_val1.csv', index_col=0)\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df = df.dropna(subset=[\"content\"])\n",
    "\n",
    "    df = df.dropna(subset=[\"content\"])\n",
    "\n",
    "    columns_to_clean = [\n",
    "    'human_trafficking', 'forced_labor', 'child_labor',\n",
    "    'sex_trafficking', 'drug_trafficking',\n",
    "    'modern_slavery', 'modern_slavery_in_supply_chain'\n",
    "    ]\n",
    "\n",
    "    for col in tqdm(columns_to_clean, desc=\"Limpando colunas de categorias\"):\n",
    "        # Converte tudo para string e min√∫sculas\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "        \n",
    "        # Substitui valores diferentes de 'yes' ou 'no' por 'no'\n",
    "        df[col] = df[col].where(df[col].isin(['yes', 'no']), 'no')\n",
    "\n",
    "    #identify entities and input placeholders\n",
    "    df['email'] = df['content'].apply(find_emails)\n",
    "    df['email'] = df['email'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['email'] = df['email'].apply(lambda x: list(set(x)) if isinstance(x, list) else x)\n",
    "    unique_emails = extract_unique_values(df, 'email')\n",
    "    df['content_no_entity'] = df['content'].apply(lambda x: replace_entity(x, unique_emails, \" [[EMAIL]] \"))\n",
    "\n",
    "    df['links'] = df['content_no_entity'].apply(find_links)\n",
    "    df['links'] = df['links'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['links'] = df['links'].apply(lambda x: list(set(x)) if isinstance(x, list) else x)\n",
    "    unique_links = extract_unique_values(df, 'links')\n",
    "    df['content_no_entity'] = df['content_no_entity'].apply(lambda x: replace_entity(x, unique_links, \" [[LINK]] \"))\n",
    "    unique_links2 =[extract_radical(url) for url in unique_links]\n",
    "    df['content_no_entity'] = df['content_no_entity'].apply(lambda x: replace_entity(x, unique_links2, \" [[LINK]] \"))\n",
    "    #----\n",
    "    df = df[df[\"content_no_entity\"].notna()].copy()\n",
    "    texts = df[\"content_no_entity\"].tolist()\n",
    "  \n",
    "    # Run batch NER\n",
    "    #docs = list(nlp.pipe(texts, batch_size=32))\n",
    "\n",
    "    # Process each doc into anonymized text + entities\n",
    "    #results = [identify_entities(doc) for doc in docs]\n",
    "\n",
    "    # Unpack results\n",
    "    #df[\"content_no_entity\"] = [r[0] for r in results]\n",
    "    #df[\"org\"] = [list(set(r[1])) for r in results]\n",
    "    #df[\"person\"] = [list(set(r[2])) for r in results]\n",
    "    #df[\"location\"] = [list(set(r[3])) for r in results]\n",
    "    #df[\"date\"] = [list(set(r[4])) for r in results]\n",
    "    #df[\"GPE\"] = [list(set(r[5])) for r in results]\n",
    "\n",
    "    \n",
    "    # ---- Load RoBERTa NER pipeline with DATE support ----\n",
    "    date_ner = pipeline(\n",
    "        \"ner\",\n",
    "        model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "        tokenizer=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "        aggregation_strategy=\"simple\",\n",
    "        device=0,        # GPU\n",
    "        batch_size=16    # Adjust based on memory\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # ---- Prepare dataset ----\n",
    "    df = df[df[\"content_no_entity\"].notna()].copy().reset_index(drop=True)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "    # ---- Run batched NER ----\n",
    "    dataset = dataset.map(identity_entities, batched=True, batch_size=16)\n",
    "    custom_stop_words = []\n",
    "\n",
    "    # ---- Convert to DataFrame if needed ----\n",
    "    df_result = dataset.to_pandas()\n",
    "    df_result['content_corrected'] = df_result['content_no_entity'].astype(str).str.lower().apply(lambda text: remove_stop_words(text,custom_stop_words))\n",
    "    df_result.to_csv(\"modern_slavery_NER_us_india.csv\", index=False)\n",
    "    # üîπ Step 1: Convert list columns into a long format using `melt`\n",
    "    df_melted = df_result.melt(id_vars=['id'], value_vars=['location', 'org', 'person', 'date', 'email', 'links', 'misc'], var_name='entity_type', value_name='entity')\n",
    "    # üîπ Step 2: Explode the 'entity' column (previously stored as lists)\n",
    "    df_melted['entity'] = df_melted['entity'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "    df_exploded = df_melted.explode('entity', ignore_index=True)\n",
    "    df_exploded = df_exploded[df_exploded[\"entity\"].astype(str) != \"[]\"]\n",
    "    df_exploded.to_csv('modern_slavery_ENTITIES_related_us_india.csv')\n",
    "\n",
    "else:\n",
    "    print(\"A file \" + str(file_path2) + ' is already placed at this path')\n",
    "    df = pd.read_csv('modern_slavery_NER_us_india.csv',index_col =0 )\n",
    "    df = df.drop_duplicates()\n",
    "    df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem das novas classes criadas:\n",
      "target\n",
      "not a modern_slavery              1752\n",
      "modern_slavery in supply chain     311\n",
      "modern_slavery                      87\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Carregar o DataFrame ---\n",
    "try:\n",
    "    df = pd.read_csv('modern_slavery_NER_us_india_val1.csv', index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo 'modern_slavery_NER_us_india_val1.csv' n√£o foi encontrado.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Limpeza e convers√£o para min√∫sculas (boa pr√°tica) ---\n",
    "df['modern_slavery'] = df['modern_slavery'].str.lower()\n",
    "df['modern_slavery_in_supply_chain'] = df['modern_slavery_in_supply_chain'].str.lower()\n",
    "\n",
    "# --- 3. Cria√ß√£o da nova coluna de r√≥tulo com a l√≥gica das tr√™s classes ---\n",
    "# Definir as condi√ß√µes\n",
    "conditions = [\n",
    "    # Condi√ß√£o 1: A escravatura moderna est√° na cadeia de abastecimento\n",
    "    (df['modern_slavery'] == 'yes') & (df['modern_slavery_in_supply_chain'] == 'yes'),\n",
    "    # Condi√ß√£o 2: √â escravatura moderna, mas n√£o na cadeia de abastecimento\n",
    "    (df['modern_slavery'] == 'yes') & (df['modern_slavery_in_supply_chain'] == 'no'),\n",
    "    # Condi√ß√£o 3: N√£o √© escravatura moderna\n",
    "    (df['modern_slavery'] == 'no')\n",
    "]\n",
    "\n",
    "# Definir os r√≥tulos correspondentes\n",
    "labels = [\n",
    "    'modern_slavery in supply chain',\n",
    "    'modern_slavery',\n",
    "    'not a modern_slavery'\n",
    "]\n",
    "\n",
    "# Usar np.select para criar a nova coluna de forma limpa e eficiente\n",
    "df['target'] = np.select(conditions, labels, default='not a modern_slavery')\n",
    "\n",
    "# --- 4. Verifica√ß√£o dos resultados ---\n",
    "print(\"Contagem das novas classes criadas:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista das colunas categ√≥ricas bin√°rias\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "# Lista das colunas categ√≥ricas bin√°rias\n",
    "columns = [\n",
    "    'human_trafficking', 'forced_labor', 'child_labor',\n",
    "    'sex_trafficking', 'modern_slavery', 'modern_slavery_in_supply_chain'\n",
    "]\n",
    "\n",
    "# Conta quantos 'yes' tem em cada coluna\n",
    "yes_counts = {col: (df[col].str.lower() == 'yes').sum() for col in columns}\n",
    "\n",
    "# Adiciona a categoria 'not_modern_slavery'\n",
    "yes_counts['not_modern_slavery'] = (df['modern_slavery'].str.lower() !=  'yes').sum()\n",
    "\n",
    "# Converte para DataFrame\n",
    "plot_df = pd.DataFrame(list(yes_counts.items()), columns=['Categoria', 'Total'])\n",
    "\n",
    "# Ordena do menor para o maior (para barras de cima pra baixo)\n",
    "plot_df = plot_df.sort_values(by='Total', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Gr√°fico de barras horizontal\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=plot_df, y='Categoria', x='Total', palette='Blues_d')\n",
    "\n",
    "# Ajuste dos n√∫meros ao lado das barras\n",
    "for index, row in plot_df.iterrows():\n",
    "    ax.text(\n",
    "        row['Total'] + max(plot_df['Total']) * 0.01,  # deslocamento proporcional\n",
    "        index,\n",
    "        str(row['Total']),\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Quantity cases by label\", fontsize=14)\n",
    "plt.xlabel(\"Number of cases\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contagem total por fonte\n",
    "total_counts = df['source'].value_counts().reset_index()\n",
    "total_counts.columns = ['source', 'total']\n",
    "\n",
    "# Contagem de modern_slavery_in_supply_chain == 'yes'\n",
    "supply_chain_counts = df[df['modern_slavery_in_supply_chain'].str.lower() == 'yes']['source'].value_counts().reset_index()\n",
    "supply_chain_counts.columns = ['source', 'modern_slavery_in_supply_chain']\n",
    "\n",
    "# Contagem de modern_slavery == 'yes'\n",
    "modern_slavery_counts = df[df['modern_slavery'].str.lower() == 'yes']['source'].value_counts().reset_index()\n",
    "modern_slavery_counts.columns = ['source', 'modern_slavery']\n",
    "\n",
    "not_modern_slavery_counts = df[df['modern_slavery'].str.lower() != 'yes']['source'].value_counts().reset_index()\n",
    "not_modern_slavery_counts.columns = ['source', 'not_modern_slavery']\n",
    "\n",
    "# Merge de todos os DataFrames\n",
    "merged = total_counts.merge(supply_chain_counts, on='source', how='left') \\\n",
    "                     .merge(modern_slavery_counts, on='source', how='left') \\\n",
    "                     .merge(not_modern_slavery_counts, on='source', how='left')\n",
    "\n",
    "# Substitui NaN por 0 e ajusta tipo\n",
    "merged.fillna(0, inplace=True)\n",
    "merged[['not_modern_slavery','modern_slavery_in_supply_chain', 'modern_slavery']]= merged[['not_modern_slavery','modern_slavery_in_supply_chain', 'modern_slavery']].astype(int)\n",
    "\n",
    "# Formato longo para o seaborn\n",
    "long_df = pd.melt(\n",
    "    merged,\n",
    "    id_vars='source',\n",
    "    value_vars=['total', 'not_modern_slavery', 'modern_slavery','modern_slavery_in_supply_chain'],\n",
    "    var_name='category',\n",
    "    value_name='count'\n",
    ")\n",
    "\n",
    "# Define cores personalizadas para cada categoria\n",
    "palette = {\n",
    "    'total': '#012345',         # Azul escuro\n",
    "    'not_modern_slavery':'#0000FF',\n",
    "    'modern_slavery': '#00BFFF',  # po azul\n",
    "    'modern_slavery_in_supply_chain': '#B0C4DE', # Azul claro\n",
    "\n",
    "}\n",
    "\n",
    "# Gr√°fico de barras lado a lado com cores e legenda colorida\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(data=long_df, x='source', y='count', hue='category', palette='Blues_d')\n",
    "\n",
    "# N√∫meros acima de cada barra\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(\n",
    "            f'{int(height)}',\n",
    "            (p.get_x() + p.get_width() / 2., height),\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=10, color='black', xytext=(0, 4),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "# T√≠tulos e legenda final\n",
    "plt.title(\"Number of cases by source and label\", fontsize=14)\n",
    "plt.ylabel(\"Number of cases\")\n",
    "plt.xlabel(\"Source\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# A legenda j√° reflete as cores das colunas por causa da paleta\n",
    "plt.legend(title='Count by category', title_fontsize=12, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold_results = []\n",
    "time_to_predict = []\n",
    "test_result = pd.DataFrame()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove weird spacing\n",
    "    return text.strip().title()  # Capitalize normally\n",
    "    \n",
    "def train_model_with_gridsearch(\n",
    "    model_name,\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    sample_weights =None\n",
    "):\n",
    "    print(f\"\\n{model_name}: Training the model...\")\n",
    "\n",
    "    # Initial pipeline fit (optional pre-training step)\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = (time.time() - start_time) * 1000\n",
    "    print(f\"Initial Training Time: {train_time:.4f} ms\")\n",
    "\n",
    "    # Grid Search\n",
    "    print(f\"\\n{model_name}: Running GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        n_jobs=-1,\n",
    "\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "    best_model = grid_search.best_estimator_     \n",
    "\n",
    "    return best_model, grid_search, train_time\n",
    "\n",
    "\n",
    "def evaluate_pipeline_model(pipeline, model_name, X_train, y_train, X_test, y_test, kf,\n",
    "                            test_result, nfold_results, time_to_predict, train_time):\n",
    "\n",
    "    print(f\"\\n--- Evaluating Model: {model_name} ---\")\n",
    "\n",
    "    # --- Cross-Validation with Weighted Metrics ---\n",
    "    print(\"Performing cross-validation (using weighted metrics where applicable)...\")\n",
    "    # Note: Accuracy scoring doesn't have a '_weighted' variant, it's inherently weighted.\n",
    "    # Use '_weighted' for recall, precision, f1 if needed during CV.\n",
    "    # Storing only recall and accuracy here as per original code, but showing how to add others.\n",
    "    recall_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='recall_weighted', n_jobs=-1)\n",
    "    # precision_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='precision_weighted', n_jobs=-1)\n",
    "    # f1_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='f1_weighted', n_jobs=-1)\n",
    "    acc_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    print(f\"Weighted Recall CV Scores: {recall_cv_scores}\")\n",
    "    print(f\"Accuracy CV Scores: {acc_cv_scores}\")\n",
    "    print(f\"Mean Weighted Recall CV: {recall_cv_scores.mean():.4f}\")\n",
    "    print(f\"Mean Accuracy CV: {acc_cv_scores.mean():.4f}\")\n",
    "\n",
    "    # --- Test Set Evaluation ---\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    start_time = time.time()\n",
    "    # Assuming pipeline is already fitted during cross-validation OR needs fitting here\n",
    "    # If cross_val_score doesn't fit the final pipeline on all train data, you need to fit it first:\n",
    "    # print(\"Fitting the pipeline on the full training data...\")\n",
    "    # pipeline.fit(X_train, y_train) # Uncomment this if pipeline isn't fitted on full data yet\n",
    "    # print(\"Fit complete.\")\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    prediction_time = (time.time() - start_time) * 1000 # Time for prediction only\n",
    "    print(f\"Prediction Time on Test Set: {prediction_time:.4f} ms\")\n",
    "\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    # classification_report already includes weighted avg by default\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # --- Calculate Weighted Metrics for Test Set ---\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    test_recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    test_f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"\\nWeighted Test Set Metrics:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Weighted Precision: {test_precision_weighted:.4f}\")\n",
    "    print(f\"Weighted Recall: {test_recall_weighted:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "\n",
    "\n",
    "    # Remove previous entries with the same model name from test_result\n",
    "    if isinstance(test_result, pd.DataFrame) and not test_result.empty:\n",
    "        test_result = test_result[test_result[\"Model\"] != model_name].copy()\n",
    "    elif not isinstance(test_result, pd.DataFrame):\n",
    "        test_result = pd.DataFrame() # Initialize if not a DataFrame\n",
    "\n",
    "    # Append new results\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        \"Accuracy\": round(test_accuracy * 100, 4),\n",
    "        \"Precision\": round(test_precision_weighted * 100, 4), # Using weighted precision\n",
    "        \"Recall\": round(test_recall_weighted * 100, 4),       # Using weighted recall\n",
    "        \"F1 Score\": round(test_f1_weighted * 100, 4),           # Using weighted F1\n",
    "        \"Time to predict\":  prediction_time \n",
    "\n",
    "    }\n",
    "    test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n",
    "\n",
    "    # --- Store CV and Timing Results ---\n",
    "    # Remove duplicates first to avoid adding the same model results multiple times if run again\n",
    "    nfold_results = [entry for entry in nfold_results if entry[\"model\"] != model_name]\n",
    "\n",
    "    # Store new fold and timing results\n",
    "    nfold_results.append({'model': model_name, 'recall_weighted': recall_cv_scores, 'accuracy': acc_cv_scores})\n",
    "    time_to_predict.append({'model': model_name, 'train_time': train_time, 'prediction_time': prediction_time})\n",
    "\n",
    "    # --- Plot Confusion Matrix ---\n",
    "    print(\"\\nPlotting Confusion Matrix...\")\n",
    "    try:\n",
    "        plot_confusion_matrix(y_test, y_pred,\n",
    "                              labels=['Negative','Positive'], # Make sure these labels match your encoding\n",
    "                              title=f'{model_name}: Confusion Matrix (Test Set)',\n",
    "                              cmap='Blues')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot confusion matrix: {e}\")\n",
    "\n",
    "\n",
    "    return test_result, nfold_results, time_to_predict , y_pred\n",
    "\n",
    "\n",
    "def evaluate_pipeline_model(pipeline, model_name, X_train, y_train, X_test, y_test, kf,\n",
    "                            test_result, nfold_results, time_to_predict, train_time, X_test_source):\n",
    "    print(f\"\\n--- Evaluating Model: {model_name} ---\")\n",
    "\n",
    "    # --- Cross-Validation with Weighted Metrics ---\n",
    "    print(\"Performing cross-validation (using weighted metrics where applicable)...\")\n",
    "    recall_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='recall_weighted', n_jobs=-1)\n",
    "    acc_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    print(f\"Weighted Recall CV Scores: {recall_cv_scores}\")\n",
    "    print(f\"Accuracy CV Scores: {acc_cv_scores}\")\n",
    "    print(f\"Mean Weighted Recall CV: {recall_cv_scores.mean():.4f}\")\n",
    "    print(f\"Mean Accuracy CV: {acc_cv_scores.mean():.4f}\")\n",
    "\n",
    "    # --- Test Set Evaluation ---\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    start_time = time.time()\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    prediction_time = (time.time() - start_time) * 1000 # Time for prediction only\n",
    "    print(f\"Prediction Time on Test Set: {prediction_time:.4f} ms\")\n",
    "\n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # --- Calculate Weighted Metrics for Test Set (Overall) ---\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    test_recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    test_f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"\\nWeighted Test Set Metrics (Overall):\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Weighted Precision: {test_precision_weighted:.4f}\")\n",
    "    print(f\"Weighted Recall: {test_recall_weighted:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {test_f1_weighted:.4f}\")\n",
    "\n",
    "    # --- Evaluate by Source ---\n",
    "    print(\"\\nEvaluating results by 'source':\")\n",
    "    unique_sources = X_test_source.unique()\n",
    "    source_evaluation_results = []\n",
    "\n",
    "    # Convert y_test to a Series with the same index as X_test_source for proper alignment\n",
    "    y_test_series = pd.Series(y_test.values, index=X_test_source.index)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test_source.index)\n",
    "\n",
    "    for source in unique_sources:\n",
    "        print(f\"\\n--- Source: {source} ---\")\n",
    "        source_indices = X_test_source[X_test_source == source].index\n",
    "        \n",
    "        y_test_source = y_test_series.loc[source_indices]\n",
    "        y_pred_source = y_pred_series.loc[source_indices]\n",
    "\n",
    "        if len(y_test_source) == 0:\n",
    "            print(f\"No samples for source '{source}' in the test set.\")\n",
    "            continue\n",
    "\n",
    "        source_accuracy = accuracy_score(y_test_source, y_pred_source)\n",
    "        source_precision_weighted = precision_score(y_test_source, y_pred_source, average='weighted', zero_division=0)\n",
    "        source_recall_weighted = recall_score(y_test_source, y_pred_source, average='weighted', zero_division=0)\n",
    "        source_f1_weighted = f1_score(y_test_source, y_pred_source, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"  Accuracy: {source_accuracy:.4f}\")\n",
    "        print(f\"  Weighted Precision: {source_precision_weighted:.4f}\")\n",
    "        print(f\"  Weighted Recall: {source_recall_weighted:.4f}\")\n",
    "        print(f\"  Weighted F1 Score: {source_f1_weighted:.4f}\")\n",
    "\n",
    "        source_evaluation_results.append({\n",
    "            'Model': model_name,\n",
    "            'Source': source,\n",
    "            \"Accuracy\": round(source_accuracy * 100, 4),\n",
    "            \"Precision\": round(source_precision_weighted * 100, 4),\n",
    "            \"Recall\": round(source_recall_weighted * 100, 4),\n",
    "            \"F1 Score\": round(source_f1_weighted * 100, 4),\n",
    "            \"Time to predict\": prediction_time # This time is for the full test set, not per-source\n",
    "        })\n",
    "\n",
    "    # Remove previous entries with the same model name from test_result\n",
    "    if isinstance(test_result, pd.DataFrame) and not test_result.empty:\n",
    "        test_result = test_result[test_result[\"Model\"] != model_name].copy()\n",
    "    elif not isinstance(test_result, pd.DataFrame):\n",
    "        test_result = pd.DataFrame() # Initialize if not a DataFrame\n",
    "\n",
    "    # Append new overall results\n",
    "    overall_results = {\n",
    "        'Model': model_name,\n",
    "        'Source': 'Overall', # Add a Source column for overall results\n",
    "        \"Accuracy\": round(test_accuracy * 100, 4),\n",
    "        \"Precision\": round(test_precision_weighted * 100, 4),\n",
    "        \"Recall\": round(test_recall_weighted * 100, 4),\n",
    "        \"F1 Score\": round(test_f1_weighted * 100, 4),\n",
    "        \"Time to predict\": prediction_time\n",
    "    }\n",
    "    test_result = pd.concat([test_result, pd.DataFrame([overall_results])], ignore_index=True)\n",
    "\n",
    "    # Append per-source results\n",
    "    test_result = pd.concat([test_result, pd.DataFrame(source_evaluation_results)], ignore_index=True)\n",
    "\n",
    "\n",
    "    # --- Store CV and Timing Results ---\n",
    "    nfold_results = [entry for entry in nfold_results if entry[\"model\"] != model_name]\n",
    "    nfold_results.append({'model': model_name, 'recall_weighted': recall_cv_scores, 'accuracy': acc_cv_scores})\n",
    "    time_to_predict = [entry for entry in time_to_predict if entry[\"model\"] != model_name] # Remove old entry if exists\n",
    "    time_to_predict.append({'model': model_name, 'train_time': train_time, 'prediction_time': prediction_time})\n",
    "\n",
    "    # --- Plot Confusion Matrix (Overall) ---\n",
    "    print(\"\\nPlotting Overall Confusion Matrix...\")\n",
    "    try:\n",
    "        plot_confusion_matrix(y_test, y_pred,\n",
    "                              labels=['Negative','Positive'],\n",
    "                              title=f'{model_name}: Confusion Matrix (Overall Test Set)',\n",
    "                              cmap='Blues')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot overall confusion matrix: {e}\")\n",
    "\n",
    "    return test_result, nfold_results, time_to_predict , y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_grid_svm = {\n",
    "    'clf__C': [0.1, 1, 10, 100],\n",
    "    'clf__kernel': ['linear'],\n",
    "    'clf__gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'clf__degree': [2, 3],  # Only relevant for 'poly' kernel\n",
    "    'clf__class_weight': [None, 'balanced'],\n",
    "    'clf__max_iter': [100, 1000, 5000],\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'clf__n_estimators': [50, 100, 200, 300, 500],\n",
    "    'clf__max_depth': [3, 6, 18],\n",
    "    'clf__learning_rate': [0.01, 0.5,0.1],\n",
    "    'clf__subsample': [0.8, 1.0],\n",
    "\n",
    "}\n",
    "param_grid_lr = {\n",
    "    'clf__penalty': ['l1', 'l2', 'elasticnet', None],     # Regularization type\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],             # Inverse of regularization strength\n",
    "    'clf__solver': ['saga','liblinear','lbfgs' ],                             # 'saga' supports all penalties\n",
    "    'clf__max_iter': [30, 50, 100]                    # Iteration limit\n",
    "}\n",
    "\n",
    "\n",
    "# Define hyperparameters for AdaBoost\n",
    "param_grid_adaboost = {\n",
    "    'clf__n_estimators': [50, 100, 200, 300],\n",
    "    'clf__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'clf__estimator': [\n",
    "        DecisionTreeClassifier(max_depth=1),\n",
    "        DecisionTreeClassifier(max_depth=2)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [50, 100, 200,300],        # Number of trees in the forest\n",
    "    'clf__max_depth': [None,5, 10, 20],        # Maximum depth of the tree\n",
    "    'clf__min_samples_split': [5, 10, 15],        # Minimum number of samples required to split an internal node\n",
    "    'clf__max_features': ['sqrt', 'log2', None],# Number of features to consider when looking for the best split\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting necessary for transformation\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\" \".join([self.lemmatizer.lemmatize(word,\"v\") for word in text.split()]) for text in X]\n",
    "\n",
    "class GloveVectorizer:\n",
    "    def __init__(self, glove, dim):\n",
    "        self.glove = glove\n",
    "        self.dim = dim\n",
    "\n",
    "    def encode(self, texts, show_progress_bar=False):\n",
    "        vectors = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_vecs = [self.glove.get(w.lower(), np.zeros(self.dim)) for w in words]\n",
    "            if word_vecs:\n",
    "                vectors.append(np.mean(word_vecs, axis=0))\n",
    "            else:\n",
    "                vectors.append(np.zeros(self.dim))\n",
    "        return np.array(vectors)\n",
    "    \n",
    "def load_glove(path):\n",
    "    glove = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector\n",
    "    return glove\n",
    "\n",
    "    \n",
    "class SentenceBERTVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', pooling='mean'):\n",
    "        self.model_name = model_name\n",
    "        self.pooling = pooling\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure input is a list of strings\n",
    "        if hasattr(X, 'tolist'):\n",
    "            texts = X.tolist()\n",
    "        elif isinstance(X, list):\n",
    "            texts = X\n",
    "        else:\n",
    "            raise ValueError(\"Input X must be a Series, list, or array of strings.\")\n",
    "\n",
    "        # Embed the text using SentenceTransformer\n",
    "        return self.model.encode(texts, batch_size=32, show_progress_bar=False)\n",
    "\n",
    "def get_only_words_from_strings(text):\n",
    "    \"\"\"\n",
    "    Removes numbers, punctuation, and special characters from a given string,\n",
    "    but keeps square brackets [].\n",
    "\n",
    "    :param text: The input string to be cleaned.\n",
    "    :return: A cleaned string with only alphabetic characters, spaces, and square brackets.\n",
    "    \"\"\"\n",
    "    # Keep only letters, spaces, and square brackets\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s\\[\\]]', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "#MULTI LAYER PERCEPTRON MODEL BUILDER\n",
    "def build_mlp(hp: HyperParameters):\n",
    "    \"\"\"Builds the Keras model for Keras Tuner.\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(hp.Int('units_dense1', min_value=128, max_value=512, step=32, default=256),\n",
    "              activation=hp.Choice('activation1', values=['relu', 'tanh'])),\n",
    "        Dropout(hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1, default=0.3)),\n",
    "        Dense(hp.Int('units_dense2', min_value=32, max_value=128, step=16, default=64),\n",
    "              activation=hp.Choice('activation2', values=['relu', 'tanh'])),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'], default='adam')\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'recall'])\n",
    "    return model\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=500,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding\n",
    "def classify(row):\n",
    "    if str(row['modern_slavery_in_supply_chain']).strip().lower() == 'yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['target'] = df.apply(classify, axis=1)\n",
    "\n",
    "#cleaning\n",
    "df['content_corrected'] = df['content_corrected'].apply(get_only_words_from_strings)\n",
    "df['content_corrected'] = df['content_corrected'].str.strip()\n",
    "df= df[['content_corrected','target','source']].dropna()\n",
    "\n",
    "#folds for validation\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 10\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# defining class weights and samples\n",
    "class_labels = np.unique(df['target'])\n",
    "class_weights_array = class_weight.compute_class_weight('balanced', classes=class_labels, y=df['target'])\n",
    "class_weights_dict = dict(zip(class_labels, class_weights_array))\n",
    "\n",
    "sample_weights = np.array([class_weights_dict[label] for label in df['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming this class works like a wrapper for SentenceTransformer\n",
    "sbert_vectorizer = SentenceBERTVectorizer()\n",
    "X_vectors = sbert_vectorizer.fit_transform(df['content_corrected'])\n",
    "y_full = df['target']\n",
    "source_full = df['source']\n",
    "\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert, source_train, source_test = train_test_split(\n",
    "    X_vectors, y_full, source_full, test_size=0.3, random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "# --- Step 3: Apply RandomUnderSampler to the TRAINING set ---\n",
    "rus = RandomUnderSampler(random_state=SEED)\n",
    "\n",
    "# Fit and resample the training data\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_bert, y_train_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "# Assuming y_train contains your training labels (e.g., 0 and 1)\n",
    "\n",
    "\n",
    "model_name = \"SVM + SBERT + undersampled\"\n",
    "pipeline_bert_svm = Pipeline([\n",
    "    ('clf', SVC(  probability=True, random_state=SEED))  # probability=True for predict_proba if needed\n",
    "])\n",
    "\n",
    "pipeline = pipeline_bert_svm\n",
    "param_grid = param_grid_svm\n",
    "\n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = 'f1'\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_svm_bert = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LR + SBERT\"\n",
    "pipeline_bert_lr = Pipeline([\n",
    "    ('clf', LogisticRegression(  random_state=SEED)) \n",
    "    #('clf', LogisticRegression( class_weight=class_weights_dict, random_state=SEED)) \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "pipeline = pipeline_bert_lr\n",
    "param_grid = param_grid_lr\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict , y_lr_sbert= evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"RF + SBERT\"\n",
    "pipeline_bert_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(  random_state=SEED)) \n",
    "])\n",
    "\n",
    "pipeline = pipeline_bert_rf\n",
    "param_grid = param_grid_rf\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_rf_sb = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights_dict = dict(zip(class_labels, class_weights_array))\n",
    "\n",
    "# Extract the weight for the positive class (assuming label '1')\n",
    "# If your positive class has a different label, adjust accordingly\n",
    "positive_class_weight = class_weights_dict.get(1, 1) # Default to 1 if not found\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "# A common heuristic is the ratio of the number of negative instances to the number of positive instances\n",
    "negative_count = np.sum(y_train == 0)\n",
    "positive_count = np.sum(y_train == 1)\n",
    "scale_pos_weight_value = negative_count / positive_count if positive_count > 0 else 1\n",
    "\n",
    "model_name = \"XGB + SBERT\"\n",
    "pipeline_xgb = Pipeline([\n",
    "    #('clf', XGBClassifier(scale_pos_weight=scale_pos_weight_value, random_state=SEED)) \n",
    "    ('clf', XGBClassifier(random_state=SEED)) \n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "pipeline = pipeline_xgb\n",
    "param_grid = param_grid_xgb\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_xgb_sb  = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Example (Your actual code might differ)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif # Example selector/scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_name = 'AdaBoost + SBERT'\n",
    "\n",
    "# Define a pipeline with AdaBoost instead of LGBM\n",
    "pipeline_adaboost = Pipeline([\n",
    "    ('clf',  AdaBoostClassifier( random_state=SEED))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Set pipeline and param grid\n",
    "pipeline = pipeline_adaboost\n",
    "param_grid = param_grid_adaboost\n",
    "\n",
    "X_train = X_train_resampled\n",
    "y_train = y_train_resampled\n",
    "\n",
    "X_test = X_test_bert\n",
    "y_test = y_test_bert \n",
    "\n",
    "\n",
    "# Train the model\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\",\n",
    "\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_result, nfold_results, time_to_predict , y_pred_ad_sb = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time=train_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dictionaries to DataFrame\n",
    "nfold_results_df = pd.DataFrame(nfold_results)\n",
    "temp = nfold_results_df[nfold_results_df.model.str.contains(\"SBERT\") == True]\n",
    "# Print column names to verify\n",
    "print(\"Column names in DataFrame:\", nfold_results_df.columns)\n",
    "\n",
    "# Ensure correct column names\n",
    "if 'model' in temp.columns and 'accuracy' in temp.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for model in temp['model'].unique():\n",
    "        # Extract accuracy scores (each value is a list)\n",
    "        model_scores = temp[temp['model'] == model]['accuracy'].explode().astype(float)\n",
    "        \n",
    "        # Plot individual accuracy scores across cross-validation folds\n",
    "        plt.plot(range(1, len(model_scores) + 1), model_scores, marker='.', linestyle='-', label=model)\n",
    "\n",
    "    plt.xlabel('Cross-validation Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Cross-validation Recall per Model')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MLP + SBERT\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from kerastuner.tuners import GridSearch  # Import GridSearch tuner\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from keras_tuner import GridSearch, Objective\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test, and test_result are already defined\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid_mlp = {\n",
    "    'units_dense1': [128, 256],\n",
    "    'activation1': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'units_dense2': [64, 128],\n",
    "    'activation2': ['relu'],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "}\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='min',\n",
    "    patience=300,\n",
    "    restore_best_weights=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Instantiate the GridSearch tuner and pass the hyperparameters directly\n",
    "tuner = GridSearch(\n",
    "    build_mlp,\n",
    "    objective=Objective('f1', direction='max'),\n",
    "    hyperparameters=param_grid_mlp, # Pass it here\n",
    "    directory='keras_tuner_gridsearch_dir',\n",
    "    project_name='mlp_sbert_gridsearch_tuning',\n",
    "    seed=SEED,\n",
    "    executions_per_trial=10,\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=300,\n",
    "             batch_size=64,\n",
    "             validation_split=0.2,\n",
    "             callbacks=[early_stop],    \n",
    "             verbose=1,\n",
    "             \n",
    "\n",
    ")\n",
    "\n",
    "# Print the search space summary (will reflect the grid)\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Print the results of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\\nBest Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Build the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    \n",
    "\n",
    ")\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "print(f\"Initial Training Time: {train_time:.4f} ms\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Initial Test Time: {train_time:.4f} ms\")\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_mlp_sb = (y_pred_probs > 0.5).astype(int).ravel()\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred_mlp_sb = (y_pred_probs > 0.5).astype(int).ravel()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion Matrix', cmap='Blues'):\n",
    "    \"\"\"Plots the confusion matrix with percentages.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=labels)\n",
    "    disp.plot(cmap=cmap, values_format='.2f')\n",
    "    plt.title(f'{title} with Percentages')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_mlp_sb,\n",
    "                        labels=['Negative', 'Positive'],\n",
    "                        title=f'{model_name}',\n",
    "                        cmap='Blues')\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_mlp_sb)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_mlp_sb, average='weighted', zero_division=0)\n",
    "\n",
    "results = {\n",
    "    'Model': model_name,\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n",
    "\n",
    "print(\"\\nTest Results with Tuned Model (Keras Tuner GridSearch):\")\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_bars(\n",
    "    df,\n",
    "    model_col='Model',\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)'\n",
    "):\n",
    "    labels = df[model_col].tolist()\n",
    "    metric1 = df[metric1_col].tolist()\n",
    "    metric2 = df[metric2_col].tolist()\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    width = 0.50  # Aumentei a largura das barras\n",
    "    gap = 0.40    # Aumentei o espa√ßo entre os grupos de barras\n",
    "    x_spaced = x * (1 + gap)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))  \n",
    "    rects1 = ax.bar(x_spaced - width/2, metric1, width, label=name_metric1)\n",
    "    rects2 = ax.bar(x_spaced + width/2, metric2, width, label=name_metric2)\n",
    "\n",
    "    def add_labels(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 5),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        fontsize=10)\n",
    "\n",
    "    add_labels(rects1)\n",
    "    add_labels(rects2)\n",
    "\n",
    "    ax.set_ylabel('Score (%)', fontsize=12)\n",
    "    ax.set_title(f'Model Comparison: {name_metric1} vs {name_metric2}', fontsize=16)\n",
    "    ax.set_xticks(x_spaced)\n",
    "    ax.set_xticklabels(labels, rotation=45, fontsize=12, ha='right')\n",
    "\n",
    "    ax.legend(title='Metrics', loc='lower left', fontsize=11, title_fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Dropout, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner import HyperParameters, Objective, GridSearch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "is_binary = 2\n",
    "model_name = 'BiLSTM + Attention + SBERT'\n",
    "# Use the specified variables\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "num_classes_bert = 2\n",
    "is_binary_bert = True\n",
    "\n",
    "def build_bilstm_attention_sbert(hp, sbert_embedding_dim, num_classes, is_binary):\n",
    "    input_layer = Input(shape=(sbert_embedding_dim,), name='sbert_input')\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(input_layer)  # Expand dims for LSTM\n",
    "\n",
    "    # BiLSTM Layer\n",
    "    lstm_units = hp.Int('lstm_units', 32, 128, step=32)\n",
    "    x = Bidirectional(LSTM(units=lstm_units, return_sequences=True, dropout=hp.Float('lstm_dropout', 0.2, 0.5, step=0.1)))(x)\n",
    "\n",
    "    # Attention Layer\n",
    "    class AttentionLayer(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),\n",
    "                                     initializer='random_normal', trainable=True)\n",
    "            super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            scores = tf.nn.softmax(tf.squeeze(tf.matmul(inputs, self.W), axis=-1), axis=1)\n",
    "            scores_expanded = tf.expand_dims(scores, axis=-1)\n",
    "            context_vector = tf.reduce_sum(inputs * scores_expanded, axis=1)\n",
    "            return context_vector\n",
    "\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    attention_output = AttentionLayer()(x)\n",
    "\n",
    "    # Dense Layers\n",
    "    intermediate_units = hp.Int('dense_units', 64, 256, step=64)\n",
    "    x = Dense(intermediate_units, activation='relu')(attention_output)\n",
    "    x = Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1))(x)\n",
    "\n",
    "    # Output Layer\n",
    "    if is_binary:\n",
    "        output_layer = Dense(1, activation='sigmoid')(x)\n",
    "        loss_fn = 'binary_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "    else:\n",
    "        output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss=loss_fn,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "\n",
    "sbert_embedding_dim = X_train.shape[1]\n",
    "\n",
    "hp_bilstm_attn = HyperParameters()\n",
    "hp_bilstm_attn.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "hp_bilstm_attn.Float('lstm_dropout', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp_bilstm_attn.Int('dense_units', min_value=64, max_value=256, step=64)\n",
    "hp_bilstm_attn.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp_bilstm_attn.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "early_stop = EarlyStopping(monitor=Objective(\"accuracy\", direction=\"max\"), patience=100, restore_best_weights=True)\n",
    "\n",
    "tuner_bilstm_attn = GridSearch(\n",
    "    lambda hp: build_bilstm_attention_sbert(hp, sbert_embedding_dim, num_classes_bert, is_binary_bert),\n",
    "    objective=Objective(\"f1_score\", direction=\"max\"),\n",
    "    max_trials=10,  # Adjust as needed\n",
    "    directory='bilstm_attn_sbert_tune',\n",
    "    project_name='bilstm_attn_sbert',\n",
    "    overwrite=True,\n",
    "    hyperparameters=hp_bilstm_attn,\n",
    "    executions_per_trial=1 # Consider increasing for more robust results\n",
    ")\n",
    "\n",
    "print(\"\\nPerforming Grid Search for BiLSTM with Attention and SBERT...\")\n",
    "tuner_bilstm_attn.search(X_train, y_train,\n",
    "                         epochs=100,  # Adjust as needed\n",
    "                         validation_split=0.1,\n",
    "                         #callbacks=[early_stop],\n",
    "                         verbose=1)\n",
    "print(\"Grid Search Finished.\")\n",
    "\n",
    "best_hps_bilstm_attn = tuner_bilstm_attn.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters for BiLSTM with Attention and SBERT:\")\n",
    "print(best_hps_bilstm_attn.values)\n",
    "\n",
    "best_model_bilstm_attn = tuner_bilstm_attn.get_best_models(num_models=1)[0]\n",
    "best_model_bilstm_attn.summary()\n",
    "\n",
    "print(\"\\nEvaluating the Best BiLSTM with Attention and SBERT Model on the Test Set...\")\n",
    "loss_bilstm_attn, acc_bilstm_attn, f1_bilstm_attn = best_model_bilstm_attn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss: {loss_bilstm_attn:.4f}\")\n",
    "print(f\"Accuracy: {acc_bilstm_attn:.4f}\")\n",
    "print(f\"F1 Score: {f1_bilstm_attn:.4f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Initial train time: {train_time:.4f} ms\")\n",
    "best_model_bilstm_attn.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "\n",
    "\n",
    "# Predict\n",
    "start_time = time.time()\n",
    "print(f\"Initial predict Time: {train_time:.4f} ms\")\n",
    "y_pred_probs_sbert = best_model_bilstm_attn.predict(X_test_bert)\n",
    "y_pred_sbert = (y_pred_probs_sbert > 0.5).astype(int).flatten() if is_binary else np.argmax(y_pred_probs_sbert, axis=1)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "# Calculate metrics\n",
    "prec_sbert, rec_sbert, f1_sbert, _ = precision_recall_fscore_support(\n",
    "    y_test_bert, y_pred_sbert, average='weighted', zero_division=0)\n",
    "\n",
    "# Save results\n",
    "best_results_sbert = {\n",
    "    'Model': 'BiLSTM + SBERT',\n",
    "    \"Accuracy\": round(acc_bilstm_attn * 100, 4),\n",
    "    \"Precision\": round(prec_sbert * 100, 4),\n",
    "    \"Recall\": round(rec_sbert * 100, 4),\n",
    "    \"F1 Score\": round(f1_sbert * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    \"Train time\": train_time,\n",
    "    'Best Hyperparameters': best_hps_bilstm_attn.values\n",
    "}\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([best_results_sbert])], ignore_index=True)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_binary = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LSTM + SBERT'\n",
    "# Use the specified variables\n",
    "X_train = X_train_resampled\n",
    "X_test = X_test_bert\n",
    "y_train = y_train_resampled\n",
    "y_test = y_test_bert \n",
    "num_classes_bert = 2\n",
    "is_binary_bert = True\n",
    "\n",
    "\n",
    "def build_lstm_sbert(hp, sbert_embedding_dim, num_classes, is_binary):\n",
    "    input_layer = tf.keras.Input(shape=(sbert_embedding_dim,))\n",
    "    x = tf.keras.layers.Dense(hp.Int('intermediate_units', 64, 256, step=64), activation='relu')(input_layer)\n",
    "    x = tf.keras.layers.Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1))(x)\n",
    "\n",
    "    if is_binary:\n",
    "        output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        loss_fn = 'binary_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "    else:\n",
    "        output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss=loss_fn,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "sbert_embedding_dim = X_train.shape[1]\n",
    "\n",
    "hp_sbert = HyperParameters()\n",
    "hp_sbert.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "hp_sbert.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp_sbert.Boolean('spatial_dropout', default=False)\n",
    "\n",
    "tuner_sbert = GridSearch(\n",
    "    lambda hp: build_lstm_sbert(hp, sbert_embedding_dim, num_classes_bert, is_binary_bert),\n",
    "    objective=Objective(\"f1_score\", direction=\"max\"),\n",
    "    max_trials=5,  # Reduced for demonstration\n",
    "    directory='lstm_sbert_gridsearch_fixed_vars',\n",
    "    project_name='text_classification_lstm_sbert_fixed_vars',\n",
    "    overwrite=True,\n",
    "    hyperparameters=hp_sbert\n",
    ")\n",
    "\n",
    "print(\"\\nPerforming Grid Search with SBERT embeddings (using fixed variables)...\")\n",
    "tuner_sbert.search(X_train, y_train, epochs=100) # Reduced epochs\n",
    "print(\"Grid Search with SBERT embeddings (using fixed variables) Finished.\")\n",
    "\n",
    "best_hps_sbert = tuner_sbert.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters for SBERT + LSTM (using fixed variables):\")\n",
    "print(f\"LSTM Units: {best_hps_sbert.get('lstm_units')}\")\n",
    "print(f\"Dropout Rate: {best_hps_sbert.get('dropout_rate')}\")\n",
    "print(f\"Spatial Dropout: {best_hps_sbert.get('spatial_dropout')}\")\n",
    "\n",
    "best_model_sbert = tuner_sbert.get_best_models(num_models=1)[0]\n",
    "best_model_sbert.summary()\n",
    "\n",
    "print(\"\\nEvaluating the Best SBERT + LSTM Model on the Test Set (using fixed variables)...\")\n",
    "\n",
    "loss, acc_sbert, f1 = best_model_sbert.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {acc_sbert:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Initial train time: {train_time:.4f} ms\")\n",
    "best_model_sbert.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "\n",
    "# Predict\n",
    "start_time = time.time()\n",
    "y_pred_probs_sbert = best_model_sbert.predict(X_test_bert)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_sbert = (y_pred_probs_sbert > 0.5).astype(int).flatten() if is_binary else np.argmax(y_pred_probs_sbert, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "prec_sbert, rec_sbert, f1_sbert, _ = precision_recall_fscore_support(\n",
    "    y_test_bert, y_pred_sbert, average='weighted', zero_division=0)\n",
    "\n",
    "# Save results\n",
    "best_results_sbert = {\n",
    "    'Model': 'LSTM + SBERT',\n",
    "    \"Accuracy\": round(acc_sbert * 100, 4),\n",
    "    \"Precision\": round(prec_sbert * 100, 4),\n",
    "    \"Recall\": round(rec_sbert * 100, 4),\n",
    "    \"F1 Score\": round(f1_sbert * 100, 4),\n",
    "    \"Time to predict\" : predict_time,\n",
    "    \"Train time\" : train_time,\n",
    "    'Best Hyperparameters': best_hps_sbert.values\n",
    "}\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([best_results_sbert])], ignore_index=True)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Recall\n",
    "plot_comparison_bars(\n",
    "    df=test_result[test_result.Model.str.contains(\"BERT\")].sort_values(by=['Recall','Accuracy'] ),\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset for Glove\n",
    "glove = load_glove(\"glove.6B.300d.txt\")  # or 300d\n",
    "\n",
    "model = GloveVectorizer(glove=glove, dim=300) \n",
    "original_indices = np.arange(len(df))\n",
    "X_glove = model.encode(df['content_corrected'], show_progress_bar=True)\n",
    "\n",
    "\n",
    "X_train_Gl = X_glove[train_indices]\n",
    "y_train_Gl = df['target'].values[train_indices]\n",
    "X_test_Gl = X_glove[final_test_indices]\n",
    "y_test_Gl = df['target'].values[final_test_indices]\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled_GL, y_resampled_GL = rus.fit_resample(X_train_Gl, y_train_Gl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SVM + GLOVE\"\n",
    "\n",
    "\n",
    "pipeline_glove_svm = Pipeline([\n",
    "    ('clf', SVC(probability=True, random_state=SEED))\n",
    "])\n",
    "pipeline = pipeline_glove_svm\n",
    "param_grid = param_grid_svm \n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict , y_pred_glove_svm= evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"LR + GLOVE undersampled\"\n",
    "pipeline_glove_lr= Pipeline([\n",
    "    ('clf', LogisticRegression(  random_state=SEED)) \n",
    "])\n",
    "\n",
    "pipeline = pipeline_glove_lr\n",
    "param_grid = param_grid_lr\n",
    "\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "\n",
    "train_time, best_model, grid_search = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_lr_gl = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"XGB + GLOVE undersampled\"\n",
    "pipeline_glove_xgb = Pipeline([\n",
    "       #('clf', XGBClassifier(scale_pos_weight=scale_pos_weight_value, random_state=SEED)) \n",
    "       ('clf', XGBClassifier( random_state=SEED)) \n",
    "\n",
    "])\n",
    "pipeline = pipeline_glove_xgb \n",
    "param_grid = param_grid_xgb\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "\n",
    "\n",
    "train_time, best_model, grid_search = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict , y_pred_xgb_gl= evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"RF + GLOVE undersampled\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline_glove_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(  random_state=SEED)) \n",
    "])\n",
    "\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "\n",
    "pipeline = pipeline_glove_rf\n",
    "param_grid = param_grid_rf\n",
    "\n",
    "train_time, best_model, grid_search = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_rf_gl = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test, and test_result are already defined\n",
    "model_name = \"MLP + GLOVE\"\n",
    "\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid_mlp = {\n",
    "    'units_dense1': [128, 256],\n",
    "    'activation1': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'units_dense2': [64, 128],\n",
    "    'activation2': ['relu'],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "}\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    patience=100,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Instantiate the GridSearch tuner and pass the hyperparameters directly\n",
    "tuner = GridSearch(\n",
    "    build_mlp,\n",
    "     objective=Objective('f1', direction='max'),\n",
    "    hyperparameters=param_grid_mlp, # Pass it here\n",
    "    directory='keras_tuner_gridsearch_dir',\n",
    "    project_name='mlp_sbert_gridsearch_tuning',\n",
    "    seed=SEED,\n",
    "    executions_per_trial=10\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=300,\n",
    "             batch_size=64,\n",
    "             validation_split=0.2,\n",
    "             callbacks=[early_stop],    \n",
    "             verbose=1\n",
    ")\n",
    "\n",
    "# Print the search space summary (will reflect the grid)\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Print the results of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\\nBest Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Build the best model\n",
    "start_time = time.time()\n",
    "print(f\"Initial Test Time: {start_time:.4f} ms\")\n",
    "best_model =  build_mlp(best_hps)\n",
    "best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop], # Include the early_stop callback\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_mlp_glove = (y_pred_probs > 0.5).astype(int).ravel()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred_mlp_glove, labels, title='Confusion Matrix', cmap='Blues'):\n",
    "    \"\"\"Plots the confusion matrix with percentages.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred_mlp_glove)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=labels)\n",
    "    disp.plot(cmap=cmap, values_format='.2f')\n",
    "    plt.title(f'{title} with Percentages')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_mlp_glove,\n",
    "                        labels=['Negative', 'Positive'],\n",
    "                        title=f'{model_name}: Best Tuned Model (Keras Tuner GridSearch) Confusion Matrix with Percentages',\n",
    "                        cmap='Blues')\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_mlp_glove)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_mlp_glove, average='weighted', zero_division=0)\n",
    "\n",
    "results = {\n",
    "    'Model': model_name,\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    \"Train time\": train_time,\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "}\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the best model\n",
    "start_time = time.time()\n",
    "print(f\"Initial Test Time: {start_time:.4f} ms\")\n",
    "best_model =  build_mlp(best_hps)\n",
    "best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop], # Include the early_stop callback\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_mlp_glove = (y_pred_probs > 0.5).astype(int).ravel()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred_mlp_glove, labels, title='Confusion Matrix', cmap='Blues'):\n",
    "    \"\"\"Plots the confusion matrix with percentages.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred_mlp_glove)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=labels)\n",
    "    disp.plot(cmap=cmap, values_format='.2f')\n",
    "    plt.title(f'{title} with Percentages')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_mlp_glove,\n",
    "                        labels=['Negative', 'Positive'],\n",
    "                        title=f'{model_name}: Best Tuned Model (Keras Tuner GridSearch) Confusion Matrix with Percentages',\n",
    "                        cmap='Blues')\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_mlp_glove)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_mlp_glove, average='weighted', zero_division=0)\n",
    "\n",
    "results = {\n",
    "    'Model': model_name,\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    \"Train time\": train_time,\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "}\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_name = 'AdaBoost + GLOVE undersampled'\n",
    "\n",
    "# Define a pipeline with AdaBoost instead of LGBM\n",
    "pipeline_adaboost = Pipeline([\n",
    "    ('clf',  AdaBoostClassifier(\n",
    "        \n",
    "        random_state=SEED))\n",
    "])\n",
    "\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "\n",
    "# Set pipeline and param grid\n",
    "pipeline = pipeline_adaboost\n",
    "param_grid = param_grid_adaboost\n",
    "\n",
    "# Train the model\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    "\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_result, nfold_results, time_to_predict , y_pred_adam_gl = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time=train_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BiLSTM + Attention + GLoVe'\n",
    "# Use the specified variables\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "num_classes = 2\n",
    "is_binary = True\n",
    "\n",
    "\n",
    "# Assuming you have pre-processed your text data into sequences of integers\n",
    "# and have a GloVe embedding matrix (embedding_matrix) and vocabulary size (vocab_size)\n",
    "# For demonstration, let's define placeholders:\n",
    "vocab_size_glove = 10000  # Replace with your actual vocabulary size\n",
    "embedding_dim_glove = 300  # Replace with your GloVe embedding dimension\n",
    "embedding_matrix_glove = np.random.rand(vocab_size_glove, embedding_dim_glove) # Replace with your actual GloVe matrix\n",
    "max_length_glove = X_train.shape[1] # Assuming X_train is padded sequences\n",
    "\n",
    "def build_bilstm_attention_glove(hp, vocab_size, embedding_dim, max_length, num_classes, is_binary, embedding_matrix):\n",
    "    input_layer = Input(shape=(max_length,), name='glove_input')\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
    "    x = Bidirectional(LSTM(units=hp.Int('lstm_units', 32, 128, step=32),\n",
    "                           return_sequences=True,\n",
    "                           dropout=hp.Float('lstm_dropout', 0.2, 0.5, step=0.1)))(embedding_layer)\n",
    "\n",
    "    class AttentionLayer(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),\n",
    "                                     initializer='random_normal', trainable=True)\n",
    "            super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            scores = tf.nn.softmax(tf.squeeze(tf.matmul(inputs, self.W), axis=-1), axis=1)\n",
    "            scores_expanded = tf.expand_dims(scores, axis=-1)\n",
    "            context_vector = tf.reduce_sum(inputs * scores_expanded, axis=1)\n",
    "            return context_vector\n",
    "\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    attention_output = AttentionLayer()(x)\n",
    "\n",
    "    intermediate_units = hp.Int('dense_units', 64, 256, step=64)\n",
    "    x = Dense(intermediate_units, activation='relu')(attention_output)\n",
    "    x = Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1))(x)\n",
    "\n",
    "    if is_binary:\n",
    "        output_layer = Dense(1, activation='sigmoid')(x)\n",
    "        loss_fn = 'binary_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "    else:\n",
    "        output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss=loss_fn,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "\n",
    "hp_bilstm_attn_glove = HyperParameters()\n",
    "hp_bilstm_attn_glove.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "hp_bilstm_attn_glove.Float('lstm_dropout', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp_bilstm_attn_glove.Int('dense_units', min_value=64, max_value=256, step=64)\n",
    "hp_bilstm_attn_glove.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp_bilstm_attn_glove.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='accuracy', patience=500, restore_best_weights=True)\n",
    "\n",
    "tuner_bilstm_attn_glove = GridSearch(\n",
    "    lambda hp: build_bilstm_attention_glove(hp, vocab_size_glove, embedding_dim_glove, max_length_glove, num_classes, is_binary, embedding_matrix_glove),\n",
    "    objective=Objective(\"f1_score\", direction=\"max\"),\n",
    "    max_trials=10,  # Adjust as needed\n",
    "    directory='bilstm_attn_glove_tune',\n",
    "    project_name='bilstm_attn_glove',\n",
    "    overwrite=True,\n",
    "    hyperparameters=hp_bilstm_attn_glove,\n",
    "    executions_per_trial=1 # Consider increasing for more robust results\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nPerforming Grid Search for BiLSTM with Attention and GloVe...\")\n",
    "tuner_bilstm_attn_glove.search(X_train, y_train,\n",
    "                             epochs=100,  # Adjust as needed\n",
    "                             validation_split=0.1,\n",
    "                             callbacks=[early_stop],\n",
    "                             verbose=1)\n",
    "print(\"Grid Search Finished.\")\n",
    "\n",
    "best_hps_bilstm_attn_glove = tuner_bilstm_attn_glove.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters for BiLSTM with Attention and GloVe:\")\n",
    "print(best_hps_bilstm_attn_glove.values)\n",
    "\n",
    "best_model_bilstm_attn_glove = tuner_bilstm_attn_glove.get_best_models(num_models=1)[0]\n",
    "best_model_bilstm_attn_glove.summary()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_bilstm_attn_glove.fit(\n",
    "    X_train,\n",
    "    y_train, # Correct y argument\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating the Best BiLSTM with Attention and GloVe Model on the Test Set...\")\n",
    "loss_bilstm_attn_glove, acc_bilstm_attn_glove, f1_bilstm_attn_glove = best_model_bilstm_attn_glove.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss: {loss_bilstm_attn_glove:.4f}\")\n",
    "print(f\"Accuracy: {acc_bilstm_attn_glove:.4f}\")\n",
    "print(f\"F1 Score: {f1_bilstm_attn_glove:.4f}\")\n",
    "\n",
    "# Predict\n",
    "start_time = time.time()\n",
    "y_pred_probs_glove = best_model_bilstm_attn_glove.predict(X_test)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_glove = (y_pred_probs_glove > 0.5).astype(int).flatten() if is_binary else np.argmax(y_pred_probs_glove, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "prec_glove, rec_glove, f1_report_glove, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_glove, average='weighted', zero_division=0)\n",
    "\n",
    "# Save results\n",
    "\n",
    "results = {\n",
    "    'Model': model_name,\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    \"Train time\": train_time,\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "}\n",
    "\n",
    "if 'test_result' not in locals():\n",
    "    test_result = pd.DataFrame()\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner import HyperParameters, Objective, GridSearch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "is_binary_bert = True\n",
    "is_binary = True\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "model_name = 'LSTM + GloVe'\n",
    "\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"glove.6B.300d.txt\") # Call the function to load the embeddings\n",
    "embedding_dim = 300\n",
    "SEED = 42\n",
    "# Assuming you have pre-processed your text data into sequences of integers\n",
    "# and have a GloVe embedding matrix (embedding_matrix) and vocabulary size (vocab_size)\n",
    "# For demonstration, let's define placeholders:\n",
    "vocab_size = 10000  # Replace with your actual vocabulary size\n",
    "embedding_dim = 300  # Replace with your GloVe embedding dimension\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim) # Replace with your actual GloVe matrix\n",
    "max_length = X_train.shape[1] # Assuming X_train is padded sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare text data for embedding layer\n",
    "tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\") # Initialize tokenizer\n",
    "tokenizer.fit_on_texts(df['content_corrected'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "max_length = 100 # Choose an appropriate max sequence length\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['content_corrected'])\n",
    "X_padded = pad_sequences(X, maxlen=max_length, padding='post', truncating='post')\n",
    "y = df['target'].values\n",
    "\n",
    "X_train_Gl, X_test_Gl, y_train_Gl, y_test_Gl = train_test_split(\n",
    "    X_padded, y, test_size=0.2, shuffle=True, random_state=SEED\n",
    ")\n",
    "\n",
    "rus = RandomUnderSampler(random_state=SEED)\n",
    "X_resampled_GL, y_resampled_GL = rus.fit_resample(X_train_Gl, y_train_Gl)\n",
    "\n",
    "# Create embedding matrix from GloVe\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Use the specified variables\n",
    "X_train = X_resampled_GL\n",
    "y_train = y_resampled_GL\n",
    "X_test = X_test_Gl\n",
    "y_test = y_test_Gl\n",
    "num_classes = 2\n",
    "is_binary = True\n",
    "\n",
    "\n",
    "def build_lstm_glove(hp, vocab_size, embedding_dim, max_length, num_classes, is_binary, embedding_matrix):\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
    "    lstm_units = hp.Int('lstm_units', 32, 128, step=32)\n",
    "    x = LSTM(lstm_units, dropout=hp.Float('dropout_rate', 0.2, 0.5, step=0.1))(embedding_layer)\n",
    "\n",
    "    if is_binary:\n",
    "        output_layer = Dense(1, activation='sigmoid')(x)\n",
    "        loss_fn = 'binary_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "    else:\n",
    "        output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        metrics = ['accuracy', F1Score(name='f1_score')]\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss=loss_fn,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model\n",
    "\n",
    "hp_glove_lstm = HyperParameters()\n",
    "hp_glove_lstm.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "hp_glove_lstm.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp_glove_lstm.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='accuracy', patience=500, restore_best_weights=True)\n",
    "\n",
    "tuner_glove_lstm = GridSearch(\n",
    "    lambda hp: build_lstm_glove(hp, vocab_size, embedding_dim, max_length, num_classes, is_binary, embedding_matrix),\n",
    "    objective=Objective(\"f1_score\", direction=\"max\"),\n",
    "    max_trials=20,\n",
    "    directory='lstm_glove_gridsearch',\n",
    "    project_name='text_classification_lstm_glove',\n",
    "    overwrite=True,\n",
    "    hyperparameters=hp_glove_lstm\n",
    ")\n",
    "\n",
    "print(\"\\nPerforming Grid Search with GloVe embeddings and LSTM...\")\n",
    "tuner_glove_lstm.search(X_train, y_train,\n",
    "                      epochs=100,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[early_stop],\n",
    "                      verbose=1)\n",
    "print(\"Grid Search with GloVe embeddings and LSTM Finished.\")\n",
    "\n",
    "best_hps_glove_lstm = tuner_glove_lstm.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters for GloVe + LSTM:\")\n",
    "print(best_hps_glove_lstm.values)\n",
    "\n",
    "best_model_glove_lstm = tuner_glove_lstm.get_best_models(num_models=1)[0]\n",
    "best_model_glove_lstm.summary()\n",
    "\n",
    "print(\"\\nEvaluating the Best GloVe + LSTM Model on the Test Set...\")\n",
    "loss_glove, acc_glove, f1_glove = best_model_glove_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss: {loss_glove:.4f}\")\n",
    "print(f\"Accuracy: {acc_glove:.4f}\")\n",
    "print(f\"F1 Score: {f1_glove:.4f}\")\n",
    "\n",
    "start_time = time.time() \n",
    "best_model_glove_lstm.fit(\n",
    "    X_train,\n",
    "    y_train, # Correct y argument\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "\n",
    "# Predict\n",
    "\n",
    "start_time = time.time() \n",
    "y_pred_probs_glove = best_model_glove_lstm.predict(X_test)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_glove = (y_pred_probs_glove > 0.5).astype(int).flatten() if is_binary else np.argmax(y_pred_probs_glove, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "prec_glove, rec_glove, f1_report_glove, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_glove, average='weighted', zero_division=0)\n",
    "\n",
    "# Save results\n",
    "\n",
    "results = {\n",
    "    'Model': model_name,\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    \"Train time\": train_time,\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "}\n",
    "\n",
    "if 'test_result' not in locals():\n",
    "    test_result = pd.DataFrame()\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Recall\n",
    "plot_comparison_bars(\n",
    "\n",
    "\n",
    "    df = test_result[\n",
    "    test_result['Model'].str.contains(\"GLOVE\")\n",
    "].sort_values(\n",
    "    by=['Recall', 'Accuracy'],\n",
    "    ascending=False\n",
    ").drop_duplicates(subset=['Model']),\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vectorize text with TF-IDF\n",
    "vectorizer = TfidfVectorizer(lowercase=True, max_features=100000, ngram_range=(1, 3))\n",
    "X_tfidf = vectorizer.fit_transform(df['content_corrected'])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_full = vectorizer.fit_transform(df['content_corrected'])\n",
    "y_full = df['target']\n",
    "source_full = df['source']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf, source_train, source_test = train_test_split(\n",
    "    X_full, y_full, source_full, test_size=0.2, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Apply undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled_tf, y_resampled_tf = rus.fit_resample(X_train_tf, y_train_tf)\n",
    "X_train = X_resampled_tf\n",
    "y_train = y_resampled_tf\n",
    "X_test = X_test_tf\n",
    "y_test = y_test_tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM + TFIDF: Training the model...\n",
      "Initial Training Time: 330.4546 ms\n",
      "\n",
      "SVM + TFIDF: Running GridSearchCV...\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "Best Parameters: {'clf__C': 10, 'clf__class_weight': None, 'clf__degree': 2, 'clf__gamma': 'scale', 'clf__kernel': 'linear', 'clf__max_iter': 100}\n",
      "Best CV Score: 0.575261324041812\n",
      "\n",
      "--- Evaluating Model: SVM + TFIDF ---\n",
      "Performing cross-validation (using weighted metrics where applicable)...\n",
      "Weighted Recall CV Scores: [0.61904762 0.61904762 0.66666667 0.61904762 0.61904762 0.66666667\n",
      " 0.61904762 0.6        0.6        0.7       ]\n",
      "Accuracy CV Scores: [0.61904762 0.61904762 0.66666667 0.61904762 0.61904762 0.66666667\n",
      " 0.61904762 0.6        0.6        0.7       ]\n",
      "Mean Weighted Recall CV: 0.6329\n",
      "Mean Accuracy CV: 0.6329\n",
      "\n",
      "Making predictions on the test set...\n",
      "Prediction Time on Test Set: 17.1475 ms\n",
      "\n",
      "Classification Report (Test Set):\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                modern_slavery       0.19      0.78      0.31        18\n",
      "modern_slavery in supply chain       0.50      0.48      0.49        64\n",
      "          not a modern_slavery       0.93      0.79      0.85       348\n",
      "\n",
      "                      accuracy                           0.74       430\n",
      "                     macro avg       0.54      0.68      0.55       430\n",
      "                  weighted avg       0.83      0.74      0.78       430\n",
      "\n",
      "\n",
      "Weighted Test Set Metrics (Overall):\n",
      "Accuracy: 0.7419\n",
      "Weighted Precision: 0.8341\n",
      "Weighted Recall: 0.7419\n",
      "Weighted F1 Score: 0.7758\n",
      "\n",
      "Evaluating results by 'source':\n",
      "\n",
      "--- Source: Courtlistener ---\n",
      "  Accuracy: 0.7507\n",
      "  Weighted Precision: 0.8474\n",
      "  Weighted Recall: 0.7507\n",
      "  Weighted F1 Score: 0.7867\n",
      "\n",
      "--- Source: IndiaLII ---\n",
      "  Accuracy: 0.7037\n",
      "  Weighted Precision: 0.8041\n",
      "  Weighted Recall: 0.7037\n",
      "  Weighted F1 Score: 0.7382\n",
      "\n",
      "Plotting Overall Confusion Matrix...\n",
      "Could not plot overall confusion matrix: The number of FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (2).\n",
      "\n",
      "Final Test Result DataFrame:\n",
      "         Model         Source  Accuracy  Precision  Recall  F1 Score  \\\n",
      "0  SVM + TFIDF        Overall     74.19      83.41   74.19     77.58   \n",
      "1  SVM + TFIDF  Courtlistener     75.07      84.74   75.07     78.67   \n",
      "2  SVM + TFIDF       IndiaLII     70.37      80.41   70.37     73.82   \n",
      "\n",
      "   Time to predict  \n",
      "0            17.15  \n",
      "1            17.15  \n",
      "2            17.15  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = \"SVM + TFIDF\"\n",
    "pipeline_svm = Pipeline([\n",
    "        ('selector', SelectKBest(score_func=chi2, k=500)), \n",
    "\n",
    "    ('clf', SVC(probability=True, random_state=SEED))\n",
    "])\n",
    "\n",
    "pipeline = pipeline_svm\n",
    "param_grid = param_grid_svm\n",
    "X_train = X_resampled_tf\n",
    "y_train = y_resampled_tf\n",
    "X_test = X_test_tf\n",
    "y_test = y_test_tf\n",
    "\n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"accuracy\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_svm_bert = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time,\n",
    "    X_test_source=source_test # Pass the source information for the test set\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Test Result DataFrame:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Source</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time to predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM + TFIDF</td>\n",
       "      <td>Overall</td>\n",
       "      <td>74.19</td>\n",
       "      <td>83.41</td>\n",
       "      <td>74.19</td>\n",
       "      <td>77.58</td>\n",
       "      <td>17.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM + TFIDF</td>\n",
       "      <td>Courtlistener</td>\n",
       "      <td>75.07</td>\n",
       "      <td>84.74</td>\n",
       "      <td>75.07</td>\n",
       "      <td>78.67</td>\n",
       "      <td>17.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM + TFIDF</td>\n",
       "      <td>IndiaLII</td>\n",
       "      <td>70.37</td>\n",
       "      <td>80.41</td>\n",
       "      <td>70.37</td>\n",
       "      <td>73.82</td>\n",
       "      <td>17.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model         Source  Accuracy  Precision  Recall  F1 Score  \\\n",
       "0  SVM + TFIDF        Overall     74.19      83.41   74.19     77.58   \n",
       "1  SVM + TFIDF  Courtlistener     75.07      84.74   75.07     78.67   \n",
       "2  SVM + TFIDF       IndiaLII     70.37      80.41   70.37     73.82   \n",
       "\n",
       "   Time to predict  \n",
       "0            17.15  \n",
       "1            17.15  \n",
       "2            17.15  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result "
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner.tuners import GridSearch  # Assuming you are using kerastuner for GridSearch\n",
    "# Assuming X_train, y_train, X_test, y_test, and test_result are already defined\n",
    "model_name = \"MLP + TFIDF\"\n",
    "X_train = X_resampled_tf\n",
    "y_train = y_resampled_tf\n",
    "X_test = X_test_tf\n",
    "y_test = y_test_tf\n",
    "\n",
    "# Define the hyperparameter space\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    patience=100,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Instantiate the GridSearch tuner and pass the hyperparameters directly\n",
    "tuner = GridSearch(\n",
    "    build_mlp,\n",
    "     objective=Objective('f1', direction='max'),\n",
    "    hyperparameters=param_grid_mlp, # Pass it here\n",
    "    directory='keras_tuner_gridsearch_dir',\n",
    "    project_name='mlp_sbert_gridsearch_tuning',\n",
    "    seed=SEED,\n",
    "    executions_per_trial=10\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=300,\n",
    "             batch_size=64,\n",
    "             validation_split=0.2,\n",
    "             callbacks=[early_stop],    \n",
    "             verbose=1\n",
    ")\n",
    "\n",
    "# Print the search space summary (will reflect the grid)\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Print the results of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\\nBest Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Build the best model\n",
    "best_model = build_mlp(best_hps)\n",
    "start_time = time.time()\n",
    "best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "train_time = (time.time() - start_time) * 1000\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "start_time = time.time()\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "predict_time = (time.time() - start_time) * 1000\n",
    "\n",
    "y_pred_mlp_tfidf = (y_pred_probs > 0.5).astype(int).ravel()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred_mlp_tfidf, labels, title='Confusion Matrix', cmap='Blues'):\n",
    "    \"\"\"Plots the confusion matrix with percentages.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred_mlp_tfidf)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=labels)\n",
    "    disp.plot(cmap=cmap, values_format='.2f')\n",
    "    plt.title(f'{title} with Percentages')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_mlp_tfidf,\n",
    "                        labels=['Negative', 'Positive'],\n",
    "                        title=f'{model_name}: Best Tuned Model (Keras Tuner GridSearch) Confusion Matrix with Percentages',\n",
    "                        cmap='Blues')\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_mlp_tfidf, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "results = {\n",
    "    'Model': model_name,\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    \"Time to predict\": predict_time,\n",
    "    \"Train time\": train_time,\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "}\n",
    "\n",
    "if 'test_result' not in locals():\n",
    "    test_result = pd.DataFrame()\n",
    "\n",
    "test_result = pd.concat([test_result, pd.DataFrame([results])], ignore_index=True)\n",
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "model_name = 'AdaBoost + TFIDF undersampled'\n",
    "\n",
    "# Define a pipeline with AdaBoost instead of LGBM\n",
    "pipeline_adaboost = Pipeline([\n",
    "    ('selector', SelectKBest(score_func=chi2, k=500)), \n",
    "    ('clf',  AdaBoostClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "\n",
    "# Set pipeline and param grid\n",
    "pipeline = pipeline_adaboost\n",
    "param_grid = param_grid_adaboost\n",
    "\n",
    "\n",
    "# Train the model\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    ")\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_svm_bert = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time,\n",
    "    X_test_source=source_test # Pass the source information for the test set\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Test Result DataFrame:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_name = 'XGB + TFIDF undersampled '\n",
    "\n",
    "# Define the ML pipeline\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('selector', SelectKBest(score_func=chi2, k=500)), \n",
    "    ('clf', XGBClassifier(random_state = 42)) \n",
    "])\n",
    "\n",
    "pipeline = pipeline_xgb\n",
    "param_grid = param_grid_xgb\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_xgb_tfidf = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LR + TFIDF '\n",
    "\n",
    "# Define the ML pipeline\n",
    "pipeline_lr = Pipeline([\n",
    "    #('selector', SelectKBest(score_func=chi2, k=500)), \n",
    "    ('clf', LogisticRegression(  random_state = 42)) \n",
    "])\n",
    "pipeline = pipeline_lr\n",
    "param_grid = param_grid_lr\n",
    "\n",
    "X_train = X_resampled_tf\n",
    "y_train = y_resampled_tf\n",
    "X_test = X_test_tf\n",
    "y_test = y_test_tf\n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = \"accuracy\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_lr_tfidf= evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time,\n",
    "    X_test_source=source_test # Pass the source information for the test set\n",
    "\n",
    "    )\n",
    "joblib.dump(best_model, 'best_lr_pipeline.pkl')\n",
    "print(\"Model pipeline saved to best_lr_pipeline.pkl\")\n",
    "print(\"\\nFinal Test Result DataFrame:\")\n",
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_name = 'RF + TFIDF '\n",
    "\n",
    "# Define the ML pipeline\n",
    "pipeline_rf = Pipeline([\n",
    "    ('selector', SelectKBest(score_func=chi2, k=500)), \n",
    "    ('clf', RandomForestClassifier( random_state=SEED)) \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "pipeline = pipeline_rf\n",
    "param_grid = param_grid_rf\n",
    "X_train = X_resampled_tf\n",
    "y_train = y_resampled_tf\n",
    "X_test = X_test_tf\n",
    "y_test = y_test_tf\n",
    "\n",
    "\n",
    "best_model, grid_search, train_time = train_model_with_gridsearch(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    scoring = 'f1'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_result, nfold_results, time_to_predict, y_pred_svm_bert = evaluate_pipeline_model(\n",
    "    pipeline=best_model,\n",
    "    model_name=model_name,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    kf=kf,\n",
    "    test_result=test_result,\n",
    "    nfold_results=nfold_results,\n",
    "    time_to_predict=time_to_predict,\n",
    "    train_time = train_time,\n",
    "    X_test_source=source_test # Pass the source information for the test set\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Test Result DataFrame:\")\n",
    "print(test_result)\n",
    "\n",
    "\n",
    "\n",
    "joblib.dump(best_model, 'best_rf_pipeline.pkl')\n",
    "print(\"Model pipeline saved to best_rf_pipeline.pkl\")\n",
    "joblib.dump(X_test_tf, 'X_test_tf.joblib')\n",
    "joblib.dump(y_test_tf, 'y_test_tf.joblib')\n",
    "joblib.dump(source_test, 'source_test.joblib')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Networks Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Recall\n",
    "plot_comparison_bars(\n",
    "\n",
    "\n",
    "    df = test_result[\n",
    "    test_result['Model'].str.contains(\"TFIDF\")\n",
    "].sort_values(\n",
    "    by=['Accuracy'],\n",
    "    ascending=False\n",
    ").drop_duplicates(subset=['Model']),\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dictionaries to DataFrame\n",
    "nfold_results_df = pd.DataFrame(nfold_results)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"Column names in DataFrame:\", nfold_results_df.columns)\n",
    "\n",
    "# Ensure correct column names\n",
    "if 'model' in nfold_results_df.columns and 'recall' in nfold_results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for model in nfold_results_df['model'].unique():\n",
    "        # Extract accuracy scores (each value is a list)\n",
    "        model_scores = nfold_results_df[nfold_results_df['model'] == model]['recall'].explode().astype(float)\n",
    "        \n",
    "        # Plot individual accuracy scores across cross-validation folds\n",
    "        plt.plot(range(1, len(model_scores) + 1), model_scores, marker='.', linestyle='-', label=model)\n",
    "\n",
    "    plt.xlabel('Cross-validation Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Cross-validation Recall per Model')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Columns 'model' or 'accuracy' not found in nfold_results_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dictionaries to DataFrame\n",
    "nfold_results_df = pd.DataFrame(nfold_results)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"Column names in DataFrame:\", nfold_results_df.columns)\n",
    "\n",
    "# Ensure correct column names\n",
    "if 'model' in nfold_results_df.columns and 'accuracy' in nfold_results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for model in nfold_results_df['model'].unique():\n",
    "        # Extract accuracy scores (each value is a list)\n",
    "        model_scores = nfold_results_df[nfold_results_df['model'] == model]['accuracy'].explode().astype(float)\n",
    "        \n",
    "        # Plot individual accuracy scores across cross-validation folds\n",
    "        plt.plot(range(1, len(model_scores) + 1), model_scores, marker='.', linestyle='-', label=model)\n",
    "\n",
    "    plt.xlabel('Cross-validation Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Cross-validation Accuracy per Model')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Columns 'model' or 'accuracy' not found in nfold_results_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.sort_values(by=\"F1 Score\").to_csv(\"courlistenercasespredict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_df = nfold_results_df[[\"model\",\"recall\"]].explode(\"recall\").groupby(\"model\").mean()*100\n",
    "accuracy_df = nfold_results_df[[\"model\",\"accuracy\"]].explode(\"accuracy\").groupby(\"model\").mean()*100\n",
    "\n",
    "recall_df.reset_index(\"model\",inplace= True)\n",
    "accuracy_df.reset_index(\"model\",inplace= True)\n",
    "\n",
    "\n",
    "comparison_df = accuracy_df.merge(recall_df, on=\"model\")\n",
    "comparison_df['model'] = comparison_df['model'].replace(\"RandomForest + TFIDF \", \"RF + TFIDF\")\n",
    "comparison_df['model'] = comparison_df['model'].replace(\"XGBClassifier + TFIDF \", \"XGB + TFIDF\")\n",
    "comparison_df['model'] = comparison_df['model'].replace(\"LIGHTGBM + SBERT\", \"LGBM + SBERT\")\n",
    "comparison_df.set_index(\"model\", inplace=True)\n",
    "\n",
    "# Accuracy vs Recall\n",
    "plot_comparison_bars(\n",
    "    df=test_result.sort_values(by=['Recall']).drop_duplicates(),\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_results = nfold_results_df.groupby(\"model\").agg({\n",
    "        \"recall\": \"mean\",\n",
    "        \"accuracy\": \"mean\"\n",
    "    }).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_name = 'LSTM '\n",
    "\n",
    "# Extract X and y\n",
    "texts = df['content_corrected'].values # Get as numpy array of strings\n",
    "labels = df['target'].values\n",
    "\n",
    "# --- Check and Prepare Labels ---\n",
    "if isinstance(labels[0], str):\n",
    "    print(\"Labels appear to be strings. Encoding labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Found {num_classes} classes: {label_encoder.classes_}\")\n",
    "    is_binary = num_classes == 2\n",
    "else:\n",
    "    print(\"Labels appear to be numerical. Assuming they start from 0.\")\n",
    "    y = labels.astype(int)\n",
    "    num_classes = len(np.unique(y))\n",
    "    is_binary = num_classes == 2\n",
    "    print(f\"Found {num_classes} classes.\")\n",
    "\n",
    "\n",
    "# --- 2. Preprocessing Text ---\n",
    "\n",
    "# Tokenization\n",
    "max_words = 5000  # Max number of words to keep in vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\") # oov_token handles words not in vocab\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1 # +1 because index 0 is reserved\n",
    "print(f\"Found {len(word_index)} unique tokens. Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Padding\n",
    "max_sequence_length = 150 # Adjust based on your text lengths (find the max or use a percentile)\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "print('Shape of data tensor (X):', X.shape)\n",
    "print('Shape of label tensor (y):', y.shape)\n",
    "\n",
    "\n",
    "# --- 3. Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #  for balanced splits in classification\n",
    "\n",
    "print('Shape of X_train tensor:', X_train.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "print('Shape of X_test tensor:', X_test.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "# --- 4. Build the LSTM Model ---\n",
    "\n",
    "embedding_dim = 128   # Dimension of the word embeddings\n",
    "lstm_units = 64      # Number of units in the LSTM layer\n",
    "dropout_rate = 0.4    # Dropout rate for regularization\n",
    "\n",
    "# --- 4. Define the Model Building Function for Keras Tuner ---\n",
    "def build_lstm(hp: HyperParameters):\n",
    "    embedding_dim = hp.Int('embedding_dim', min_value=64, max_value=256, step=32)\n",
    "    lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    spatial_dropout = hp.Boolean('spatial_dropout', default=False)\n",
    "\n",
    "    model = Sequential(name=\"LSTM_Tuned\")\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                      output_dim=embedding_dim,\n",
    "                      input_length=max_sequence_length,\n",
    "                      mask_zero=True))\n",
    "    if spatial_dropout:\n",
    "        model.add(SpatialDropout1D(dropout_rate))\n",
    "    model.add(LSTM(units=lstm_units,\n",
    "                      dropout=dropout_rate,\n",
    "                      recurrent_dropout=dropout_rate))\n",
    "    if is_binary:\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    if is_binary:\n",
    "        loss_function = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss_function = 'sparse_categorical_crossentropy'\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_function,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- 5. Define the Hyperparameter Space for Grid Search ---FBV\n",
    "hp = HyperParameters()\n",
    "hp.Int('embedding_dim', min_value=64, max_value=256, step=32)\n",
    "hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp.Boolean('spatial_dropout', default=False)\n",
    "\n",
    "# --- 6. Instantiate the Grid Search Tuner ---\n",
    "tuner = GridSearch(\n",
    "    build_lstm,\n",
    "    objective='accuracy',  # Metric to optimize\n",
    "    max_trials=20,             # Number of different hyperparameter combinations to try\n",
    "    directory='lstm_gridsearch', # Directory to save results\n",
    "    project_name='text_classification_lstm',\n",
    "    overwrite=True,            # Overwrite existing runs if the project name is the same\n",
    "    hyperparameters=hp        # Explicitly pass the hyperparameter space\n",
    ")\n",
    "\n",
    "# --- 7. Perform the Grid Search ---\n",
    "print(\"\\nPerforming Grid Search...\")\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=100,             # Number of epochs to train each model configuration\n",
    "             )\n",
    "\n",
    "print(\"Grid Search Finished.\")\n",
    "\n",
    "# --- 8. Get the Best Hyperparameters and Model ---\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"Embedding Dimension: {best_hps.get('embedding_dim')}\")\n",
    "print(f\"LSTM Units: {best_hps.get('lstm_units')}\")\n",
    "print(f\"Dropout Rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"Spatial Dropout: {best_hps.get('spatial_dropout')}\")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()\n",
    "\n",
    "# --- 9. Evaluate the Best Model on the Test Set ---\n",
    "print(\"\\nEvaluating the Best Model on the Test Set...\")\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss of the Best Model: {loss:.4f}')\n",
    "print(f'Test Accuracy of the Best Model: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_test, y_pred,\n",
    "    labels=['Negative', 'Positive'],\n",
    "    title=f'{model_name}: Confusion Matrix with Percentages',\n",
    "    cmap='Blues'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(word_counts.values())\n",
    "cumulative_frequency = 0\n",
    "selected_words = 0\n",
    "threshold_percentage = 0.95\n",
    "\n",
    "for i, (word, count) in enumerate(most_common):\n",
    "    cumulative_frequency += count\n",
    "    if cumulative_frequency / total_words >= threshold_percentage:\n",
    "        selected_words = i + 1\n",
    "        break\n",
    "\n",
    "print(f\"Number of words covering {threshold_percentage*100:.2f}% of the vocabulary: {selected_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'BiLSTM'\n",
    "\n",
    "# --- 1. Extract X and y ---\n",
    "texts = df['content_corrected'].astype(str).values\n",
    "labels = df['target'].values\n",
    "\n",
    "# --- Check and Prepare Labels ---\n",
    "if isinstance(labels[0], str):\n",
    "    print(\"Labels appear to be strings. Encoding labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"Found {num_classes} classes: {label_encoder.classes_}\")\n",
    "    is_binary = num_classes == 2\n",
    "else:\n",
    "    print(\"Labels appear to be numerical. Assuming they start from 0.\")\n",
    "    y = labels.astype(int)\n",
    "    num_classes = len(np.unique(y))\n",
    "    is_binary = num_classes == 2\n",
    "    print(f\"Found {num_classes} classes.\")\n",
    "\n",
    "# --- 2. Preprocessing Text ---\n",
    "max_words = 5000 \n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "actual_vocab_size = max_words\n",
    "print(f\"Found {len(word_index)} unique tokens. Using vocab size: {actual_vocab_size}\")\n",
    "\n",
    "max_sequence_length = 150\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "print('Shape of data tensor (X):', X.shape)\n",
    "print('Shape of label tensor (y):', y.shape)\n",
    "\n",
    "# --- 3. Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train= rus.fit_resample(X_train, y_train)\n",
    "print('Shape of X_train tensor:', X_train.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "print('Shape of X_test tensor:', X_test.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)\n",
    "\n",
    "def build_model(hp):\n",
    "    embedding_dim = hp.Int('embedding_dim', min_value=64, max_value=256, step=32)\n",
    "    lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    spatial_dropout_rate = hp.Float('spatial_dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    intermediate_units = hp.Int('intermediate_units', min_value=32, max_value=128, step=32)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    # Input Layer\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32', name='input_sequence')\n",
    "\n",
    "    # Embedding Layer\n",
    "    embedded_sequences = Embedding(input_dim=actual_vocab_size,\n",
    "                                   output_dim=embedding_dim,\n",
    "                                   input_length=max_sequence_length,\n",
    "                                   mask_zero=True,\n",
    "                                   name='embedding')(sequence_input)\n",
    "\n",
    "    # Spatial Dropout\n",
    "    spatial_dropout = SpatialDropout1D(spatial_dropout_rate, name='spatial_dropout')(embedded_sequences)\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm_out = Bidirectional(LSTM(units=lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True),\n",
    "                               name='bidirectional_lstm')(spatial_dropout)\n",
    "\n",
    "    # Attention Layer\n",
    "    class AttentionLayer(Layer):\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(name='attention_weight',\n",
    "                                     shape=(input_shape[-1], 1),\n",
    "                                     initializer='uniform',\n",
    "                                     trainable=True)\n",
    "        def call(self, inputs):\n",
    "            scores = tf.matmul(inputs, self.W)\n",
    "            scores = tf.squeeze(scores, axis=-1)\n",
    "            weights = tf.nn.softmax(scores, axis=1)\n",
    "            weighted_output = inputs * tf.expand_dims(weights, axis=-1)\n",
    "            context_vector = tf.reduce_sum(weighted_output, axis=1)\n",
    "            return context_vector, weights\n",
    "\n",
    "    attention_out, _ = AttentionLayer(name='attention_layer')(bilstm_out)\n",
    "\n",
    "    dense = Dense(units=intermediate_units, activation='relu', name='intermediate_dense')(attention_out)\n",
    "    dropout_final = Dropout(dropout_rate, name='final_dropout')(dense)\n",
    "\n",
    "    # Output\n",
    "    if is_binary:\n",
    "        output_layer = Dense(1, activation='sigmoid', name='output_layer')(dropout_final)\n",
    "    else:\n",
    "        output_layer = Dense(num_classes, activation='softmax', name='output_layer')(dropout_final)\n",
    "\n",
    "    # ‚úÖ Model defined BEFORE compile\n",
    "    model = Model(inputs=sequence_input, outputs=[output_layer, _], name='BiLSTM_Attention_Classifier')\n",
    "\n",
    "    # Compile\n",
    "    if is_binary:\n",
    "        loss_function = 'binary_crossentropy'\n",
    "        metrics_list = [F1Score(name='f1_score'), None]\n",
    "    else:\n",
    "        loss_function = 'sparse_categorical_crossentropy'\n",
    "        metrics_list = [F1Score(name='f1_score'), None]\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=[loss_function, None],\n",
    "        metrics=metrics_list\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Update your hp definition\n",
    "hp = HyperParameters()\n",
    "hp.Int('embedding_dim', min_value=64, max_value=256, step=32)\n",
    "hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
    "hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp.Float('spatial_dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "hp.Int('intermediate_units', min_value=32, max_value=128, step=32)\n",
    "hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "hp.Choice('max_sequence_length', values=[50, 100, 150, 200])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_output_layer_accuracy',\n",
    "    mode='max',\n",
    "    patience=500,\n",
    "    restore_best_weights=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# --- Update the GridSearch tuner to optimize for F1 Score ---\n",
    "tuner = GridSearch(\n",
    "    build_model,\n",
    "    objective=Objective('output_layer_f1_score', direction='max'),  # <<< changed here\n",
    "    max_trials=100,\n",
    "    executions_per_trial=3,\n",
    "    directory='gridsearch_dir',\n",
    "    project_name='bilstm_attention_gridsearch',\n",
    "    hyperparameters=hp\n",
    ")\n",
    "\n",
    "\n",
    "# --- 7. Perform the Grid Search ---\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "tuner.search(X_train,  [y_test, np.zeros((y_test.shape[0], X_test.shape[1]))],\n",
    "             epochs=300, # Reduced epochs for demonstration\n",
    "             validation_split=0.1,\n",
    "             #callbacks=[EarlyStopping(monitor='val_output_layer_accuracy', patience=2, restore_best_weights=True)],\n",
    "             verbose=1)\n",
    "print(\"Grid Search Finished.\")\n",
    "\n",
    "# --- 8. Get the Best Hyperparameters and Model ---\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_hps.values)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "best_model.fit(\n",
    "    X_train,\n",
    "     [y_train, np.zeros((y_train.shape[0], X_train.shape[1]))],\n",
    "    epochs=2000,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)\n",
    "\n",
    "y_pred_probs, attention_weights = best_model.predict(X_test)\n",
    "\n",
    "if is_binary:\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "else:\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "best_results = {\n",
    "    'Model': 'BiLSTM',\n",
    "    \"Accuracy\": round(acc * 100, 4),\n",
    "    \"Precision\": round(prec * 100, 4),\n",
    "    \"Recall\": round(rec * 100, 4),\n",
    "    \"F1 Score\": round(f1 * 100, 4),\n",
    "    'Best Hyperparameters': best_hps.values\n",
    "}\n",
    "\n",
    "print(best_results)\n",
    "\n",
    "# Assuming 'test_result' DataFrame exists from your original code\n",
    "# For demonstration, let's initialize it if it doesn't\n",
    "if 'test_result' not in locals():\n",
    "    test_result = pd.DataFrame()\n",
    "\n",
    "# Create a DataFrame from the best results dictionary\n",
    "new_row_df = pd.DataFrame([best_results])\n",
    "\n",
    "# Concatenate the existing test_result DataFrame with the new row DataFrame\n",
    "test_result = pd.concat([test_result, new_row_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nUpdated Test Results DataFrame:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_comparison_bars(\n",
    "    df,\n",
    "    model_col='Model',\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)',\n",
    "    sort_by='Accuracy',\n",
    "    baseline_acc = 75\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced side-by-side bar chart comparing two metrics across models,\n",
    "    with automatic mean baselines for both.\n",
    "    \"\"\"\n",
    "    # --- Sort Data ---\n",
    "    df = df.sort_values(by=sort_by, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    labels = df[model_col].tolist()\n",
    "    metric1 = df[metric1_col].tolist()\n",
    "    metric2 = df[metric2_col].tolist()\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.7   # Wider bars\n",
    "    gap = 0.60     # More spacing\n",
    "    x_spaced = x * (1 + gap)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(22, 10))\n",
    "\n",
    "    # Bar colors\n",
    "    colors = ['#4E79A7', '#F28E2B']\n",
    "\n",
    "    # Plot bars\n",
    "    rects1 = ax.bar(x_spaced - width/2, metric1, width, label=name_metric1, color=colors[0])\n",
    "    rects2 = ax.bar(x_spaced + width/2, metric2, width, label=name_metric2, color=colors[1])\n",
    "\n",
    "    # Add labels on top of bars\n",
    "    def add_labels(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 8),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        fontsize=10,\n",
    "                        path_effects=[pe.withStroke(linewidth=2, foreground=\"white\")])\n",
    "\n",
    "    add_labels(rects1)\n",
    "    add_labels(rects2)\n",
    "\n",
    "    # Axis styling\n",
    "    ax.set_ylabel('Score (%)', fontsize=13)\n",
    "    ax.set_title(f'Model Comparison: {name_metric1} vs {name_metric2}', fontsize=16)\n",
    "    ax.set_xticks(x_spaced)\n",
    "    ax.set_xticklabels(labels, rotation=30, ha='right', fontsize=11)\n",
    "    ax.set_ylim(0, max(max(metric1), max(metric2)) + 10)\n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- Baselines: Mean Accuracy and Mean Recall ---\n",
    "\n",
    "    ax.axhline(\n",
    "        y=baseline_acc,\n",
    "        color=colors[0],\n",
    "        linestyle='--',\n",
    "        linewidth=1.5,\n",
    "        label=f'Mean {name_metric1}: ({baseline_acc:.2f}%)'\n",
    "    )\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(title='Metrics & Baselines', loc='lower left', fontsize=11, title_fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.read_csv('model_comparison_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result  = test_result[test_result['Model'] != 'BiLSTM with GloVe'].reset_index(drop=True)\n",
    "test_result['Model'] = test_result['Model'].str.replace('undersampled', '').str.strip()\n",
    "\n",
    "# Accuracy vs Recall\n",
    "plot_comparison_bars(\n",
    "    df=test_result.sort_values(by=['Recall'],ascending=False).drop_duplicates(subset=['Model']),\n",
    "    metric1_col='Accuracy',\n",
    "    metric2_col='Recall',\n",
    "    name_metric1='Accuracy (%)',\n",
    "    name_metric2='Recall (%)', \n",
    "    baseline_acc = test_result['Accuracy'].mean(),\n",
    "    sort_by='Accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result['Model'] = test_result['Model'].str.replace('undersampled', '', regex=False).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, for safety: strip spaces\n",
    "test_result['Model'] = test_result['Model'].str.strip()\n",
    "\n",
    "# Then, keep only the best (highest Accuracy) per Model\n",
    "test_result_cleaned = test_result.sort_values('Accuracy', ascending=False).drop_duplicates(subset=['Model'], keep='first').reset_index(drop=True)\n",
    "\n",
    "test_result_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:new_env_modern_slavery]",
   "language": "python",
   "name": "conda-env-new_env_modern_slavery-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
