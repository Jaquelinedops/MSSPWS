{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04866f49-ae73-4d9b-bbf8-7f8cbe717e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Usando o Adam compat√≠vel:\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from sklearn.utils import resample \n",
    "\n",
    "\n",
    "# Suprimir avisos\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "# --- Defini√ß√µes de Fun√ß√µes Auxiliares (CI e Avalia√ß√£o) ---\n",
    "def ci95(data):\n",
    "    \"\"\"Calcula o Intervalo de Confian√ßa de 95% assumindo distribui√ß√£o normal.\"\"\"\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = np.std(data) / np.sqrt(len(data))\n",
    "    # Z-score para 95% (1.96)\n",
    "    lower = mean - 1.96 * std_err\n",
    "    upper = mean + 1.96 * std_err\n",
    "    return (lower, upper)\n",
    "\n",
    "def bootstrap_ci(y_true, y_pred, metric_func, n_iterations=1000, alpha=0.95):\n",
    "    \"\"\"Calcula o CI de 95% usando Bootstrap para m√©tricas n√£o gaussianas (e.g., F1).\"\"\"\n",
    "    n_size = len(y_true)\n",
    "    scores = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.randint(0, n_size, n_size)\n",
    "        y_true_sample = y_true[indices]\n",
    "        y_pred_sample = y_pred[indices]\n",
    "        \n",
    "        try:\n",
    "            score = metric_func(y_true_sample, y_pred_sample, average='weighted', zero_division=0)\n",
    "            scores.append(score)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    if not scores:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    p = ((1.0 - alpha) / 2.0) * 100\n",
    "    lower = np.percentile(scores, p)\n",
    "    p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n",
    "    upper = np.percentile(scores, p)\n",
    "    \n",
    "    return (lower, upper)\n",
    "\n",
    "def evaluate_roberta_model(model_name, y_test, y_pred, pred_time_ms, train_time, cv_scores, test_result):\n",
    "    \"\"\"Consolida os resultados de CV e Teste Final.\"\"\"\n",
    "    \n",
    "    # 1. Resultados do Teste Final\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # 2. Consolida√ß√£o das M√©tricas de CV\n",
    "    cv_mean = {k: np.mean(v) for k, v in cv_scores.items()}\n",
    "    cv_std = {k: np.std(v) for k, v in cv_scores.items()}\n",
    "    cv_ci = {k: ci95(v) for k, v in cv_scores.items()} # Usa ci95 simples para os folds\n",
    "    \n",
    "    final_results = {\n",
    "        'Model': model_name,\n",
    "        'Train Time (s)': round(train_time, 2),\n",
    "        'Predict Time (ms/sample)': round(pred_time_ms / len(y_test), 2),\n",
    "        'Accuracy (Test)': round(acc * 100, 4),\n",
    "        'F1 Score (Test)': round(f1 * 100, 4),\n",
    "        \n",
    "        'CV_Acc_Mean': round(cv_mean['accuracy'] * 100, 4),\n",
    "        'CV_Acc_Std': round(cv_std['accuracy'] * 100, 4),\n",
    "        'CV_Acc_CI95': f\"({round(cv_ci['accuracy'][0]*100, 2)}, {round(cv_ci['accuracy'][1]*100, 2)})\",\n",
    "        \n",
    "        'CV_F1_Mean': round(cv_mean['f1_weighted'] * 100, 4),\n",
    "        'CV_F1_Std': round(cv_std['f1_weighted'] * 100, 4),\n",
    "        'CV_F1_CI95': f\"({round(cv_ci['f1_weighted'][0]*100, 2)}, {round(cv_ci['f1_weighted'][1]*100, 2)})\",\n",
    "    }\n",
    "    \n",
    "    # Adicionar ao DataFrame de resultados\n",
    "    return pd.concat([test_result, pd.DataFrame([final_results])], ignore_index=True)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "PATH_DATA = './'\n",
    "\n",
    "## --- 1. Carregamento de Dados e √çndices do Disco ---\n",
    "print(\"--- 1. Carregando Dados do Disco ---\")\n",
    "try:\n",
    "    # Carregar Dados Reamostrados (Usados para CV e Treinamento Final)\n",
    "    df_train_resampled = pd.read_csv(f'{PATH_DATA}train_resampled_full.csv')\n",
    "    X_train_resampled = df_train_resampled['content_corrected']\n",
    "    y_train_resampled = df_train_resampled['target']\n",
    "    \n",
    "    # Carregar Dados de Teste (Usados para Avalia√ß√£o Final)\n",
    "    df_test_original = pd.read_csv(f'{PATH_DATA}test_original_full.csv')\n",
    "    X_test_original = df_test_original['content_corrected']\n",
    "    y_test_original = df_test_original['target']\n",
    "\n",
    "    # Carregar √çndices do K-Fold (Base Reamostrada)\n",
    "    file_path_kfolds = f'{PATH_DATA}kfolds_resampled_indices.pkl'\n",
    "    with open(file_path_kfolds, 'rb') as f:\n",
    "        loaded_kfolds_indices = pickle.load(f)\n",
    "        \n",
    "    print(\"‚úÖ Todos os dados e √≠ndices carregados com sucesso.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå ERRO: Arquivo n√£o encontrado. Certifique-se de que os dados foram salvos na pasta './'. Detalhe: {e}\")\n",
    "    exit()\n",
    "\n",
    "## --- 2. Configura√ß√£o Global e Prepara√ß√£o de Vari√°veis ---\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "num_classes = len(np.unique(y_train_resampled))\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "PATIENCE = 10 \n",
    "K_FOLDS = len(loaded_kfolds_indices)\n",
    "\n",
    "# Inicializa√ß√£o dos containers de resultados\n",
    "cv_scores = {\n",
    "    \"accuracy\": [], \"precision_weighted\": [], \"recall_weighted\": [], \"f1_weighted\": []\n",
    "}\n",
    "total_cv_train_time = 0.0\n",
    "# Inicializa√ß√£o do DataFrame de resultados\n",
    "test_result = pd.DataFrame() \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Prepara√ß√£o dos dados de CV (Base Reamostrada)\n",
    "X_CV_content = X_train_resampled.tolist() # Lista de strings para tokeniza√ß√£o\n",
    "y_CV = y_train_resampled.values # Array numpy de labels\n",
    "\n",
    "# Prepara√ß√£o dos dados de Teste (Base Original)\n",
    "X_test_content = X_test_original.tolist()\n",
    "y_test = y_test_original.values \n",
    "\n",
    "print(f\"Iniciando CV de {K_FOLDS} folds em {len(X_CV_content)} amostras reamostradas.\")\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "## --- 3. Loop de Cross-Validation (K-Fold CV) ---\n",
    "print(\"\\n--- 3. Iniciando a valida√ß√£o cruzada nos dados reamostrados ---\")\n",
    "for fold_num, (train_index, val_index) in enumerate(loaded_kfolds_indices):\n",
    "    print(f\"\\n--- Fold {fold_num + 1}/{K_FOLDS} ---\")\n",
    "\n",
    "    # A. Obter dados de treino e valida√ß√£o para o fold atual (do conte√∫do da lista/array)\n",
    "    X_train_fold_content = [X_CV_content[i] for i in train_index]\n",
    "    y_train_fold = y_CV[train_index]\n",
    "    \n",
    "    X_val_fold_content = [X_CV_content[i] for i in val_index]\n",
    "    y_val_fold = y_CV[val_index]\n",
    "    \n",
    "    # B. Tokeniza√ß√£o dos dados do Fold\n",
    "    train_encodings = tokenizer(X_train_fold_content, max_length=128, padding='max_length', truncation=True, return_tensors='tf')\n",
    "    X_train_fold_ids = train_encodings['input_ids'].numpy()\n",
    "    X_train_fold_mask = train_encodings['attention_mask'].numpy()\n",
    "\n",
    "    val_encodings = tokenizer(X_val_fold_content, max_length=128, padding='max_length', truncation=True, return_tensors='tf')\n",
    "    X_val_fold_ids = val_encodings['input_ids'].numpy()\n",
    "    X_val_fold_mask = val_encodings['attention_mask'].numpy()\n",
    "    \n",
    "    # C. Re-instanciar e compilar o modelo\n",
    "    tf.keras.backend.clear_session() \n",
    "    model = TFRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    \n",
    "    # üåü CORRE√á√ÉO: Usando tf.keras.optimizers.Adam explicitamente para resolver o AttributeError\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) \n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # D. Preparar Datasets do Fold\n",
    "    train_fold_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': X_train_fold_ids, 'attention_mask': X_train_fold_mask}, y_train_fold)\n",
    "    ).shuffle(len(y_train_fold)).batch(BATCH_SIZE)\n",
    "\n",
    "    val_fold_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': X_val_fold_ids, 'attention_mask': X_val_fold_mask}, y_val_fold)\n",
    "    ).batch(BATCH_SIZE)\n",
    "\n",
    "    # E. Fine-tune o Modelo no Fold\n",
    "    early_stop_fold = EarlyStopping(monitor='val_accuracy', mode='max', patience=PATIENCE, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    start_time_fold = time.time()\n",
    "    model.fit(train_fold_dataset, validation_data=val_fold_dataset, epochs=1000, callbacks=[early_stop_fold], verbose=0) \n",
    "    train_time_fold = time.time() - start_time_fold\n",
    "    total_cv_train_time += train_time_fold\n",
    "    \n",
    "    # F. Avalia√ß√£o no Fold de Valida√ß√£o\n",
    "    y_val_pred_logits = model.predict(val_fold_dataset, verbose=0).logits\n",
    "    y_val_pred = np.argmax(y_val_pred_logits, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_val_fold, y_val_pred)\n",
    "    prec = precision_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    cv_scores[\"accuracy\"].append(acc)\n",
    "    cv_scores[\"precision_weighted\"].append(prec)\n",
    "    cv_scores[\"recall_weighted\"].append(rec)\n",
    "    cv_scores[\"f1_weighted\"].append(f1)\n",
    "    \n",
    "    print(f\"M√©tricas do Fold {fold_num+1} (Tempo: {train_time_fold:.2f} s): Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cross-validation Conclu√≠da.\")\n",
    "average_cv_train_time = total_cv_train_time / K_FOLDS\n",
    "\n",
    "## --- 4. Treinamento do Modelo Final (Usando X_train_resampled + y_train_resampled) ---\n",
    "\n",
    "print(\"\\n--- 4. Re-treinando Modelo Final na Base Reamostrada Completa ---\")\n",
    "\n",
    "# A. Tokeniza√ß√£o Final (J√° que os dados reamostrados j√° estavam em disco)\n",
    "final_train_encodings = tokenizer(X_CV_content, max_length=128, padding='max_length', truncation=True, return_tensors='tf')\n",
    "X_final_train_ids = final_train_encodings['input_ids'].numpy()\n",
    "X_final_train_mask = final_train_encodings['attention_mask'].numpy()\n",
    "y_final_train = y_CV\n",
    "\n",
    "test_encodings = tokenizer(X_test_content, max_length=128, padding='max_length', truncation=True, return_tensors='tf')\n",
    "X_final_test_ids = test_encodings['input_ids'].numpy()\n",
    "X_final_test_mask = test_encodings['attention_mask'].numpy()\n",
    "y_final_test = y_test\n",
    "\n",
    "# B. Recria√ß√£o e compila√ß√£o do modelo final\n",
    "tf.keras.backend.clear_session()\n",
    "final_model = TFRobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "# üåü CORRE√á√ÉO: Usando tf.keras.optimizers.Adam explicitamente\n",
    "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=loss, metrics=metrics)\n",
    "\n",
    "# C. Preparar Datasets de treino e teste finais\n",
    "final_train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': X_final_train_ids, 'attention_mask': X_final_train_mask}, y_final_train)\n",
    ").shuffle(len(y_final_train)).batch(BATCH_SIZE)\n",
    "\n",
    "final_test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': X_final_test_ids, 'attention_mask': X_final_test_mask}, y_final_test)\n",
    ").batch(BATCH_SIZE)\n",
    "\n",
    "# D. Treinamento\n",
    "early_stop_final = EarlyStopping(\n",
    "    monitor='val_accuracy', mode='max', patience=PATIENCE, restore_best_weights=True, verbose=1)\n",
    "\n",
    "start_time_final_train = time.time()\n",
    "final_model.fit(\n",
    "    final_train_dataset,\n",
    "    validation_data=final_test_dataset, \n",
    "    epochs=1000,\n",
    "    callbacks=[early_stop_final],\n",
    "    verbose=1)\n",
    "final_train_time = time.time() - start_time_final_train\n",
    "\n",
    "## --- 5. Avalia√ß√£o Final (Test Set Original) ---\n",
    "print(\"\\n--- 5. Predi√ß√£o e Avalia√ß√£o no Conjunto de Teste Original ---\")\n",
    "start_time_predict = time.time()\n",
    "predictions = final_model.predict(final_test_dataset, verbose=0)\n",
    "pred_time_ms = (time.time() - start_time_predict) * 1000\n",
    "\n",
    "y_pred_logits = predictions.logits\n",
    "y_pred_test = np.argmax(y_pred_logits, axis=1)\n",
    "\n",
    "## --- 6. Chamar a Fun√ß√£o de Avalia√ß√£o e Salvar Resultados ---\n",
    "model_display_name = f'RoBERTa (base) {K_FOLDS}-fold CV (Reamostrado)'\n",
    "\n",
    "test_result = evaluate_roberta_model(\n",
    "    model_name=model_display_name,\n",
    "    y_test=y_final_test, \n",
    "    y_pred=y_pred_test, \n",
    "    pred_time_ms=pred_time_ms, \n",
    "    train_time=final_train_time,\n",
    "    cv_scores=cv_scores, \n",
    "    test_result=test_result,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Resultados Finais Consolidados (M√©dia CV, Desvio Padr√£o e CI 95%) ---\")\n",
    "print(test_result.T) \n",
    "\n",
    "## --- 7. Salvar Modelo Final ---\n",
    "save_directory = \"./roberta_classifier_final_cv_resampled\"\n",
    "final_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"\\nModelo final e tokenizer salvos em {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bf57b-7f6f-44e4-b304-3346e3ef9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install \"tensorflow==2.15.0\" \"keras==2.15.0\" \"transformers==4.35.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0d596-8882-4cb9-8562-5b0e440828a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tf-keras\n",
    "!pip uninstall keras\n",
    "!pip uninstall tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9050a04-e1b7-4f2c-ae51-35558ccf23c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
